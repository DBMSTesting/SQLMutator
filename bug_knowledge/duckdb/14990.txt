{
    "sql": "create table data as select *, submission_date_time::date as date, current_timestamp as extracted_at from '<put a big file here (mine is ~400MB ndjson)>'; copy (from data) to 's3://{{somewhere}}/' (format parquet, partition_by (date, extracted_at), compression zstd, overwrite_or_ignore); copy (with deduped as (select distinct on (id) * from 's3://{{output of part 1}}/date={{date}}/**/*.parquet' order by extracted_at desc) select * from deduped order by submission_date_time) to 's3://{{somewhere else}}/data_0.parquet' (compression zstd, overwrite_or_ignore);",
    "Affected Elements": "OutOfMemoryException, InvalidInputException",
    "Root Cause Analysis": "The root cause appears to be the inability of DuckDB to handle large data uploads in constrained memory environments, leading to incomplete writes and subsequently malformed parquet files."
}