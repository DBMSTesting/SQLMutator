{
    "pattern": "CREATE TABLE data AS SELECT <select_parameter>, submission_date_time::<date_parameter> AS date, current_timestamp AS <current_timestamp_parameter> FROM '<put a big file here (mine is ~400MB ndjson)> '; COPY (FROM data) TO '<s3://{{somewhere}}/>' (FORMAT <format_parameter>, PARTITION_BY (date, extracted_at), COMPRESSION <compression_parameter>, OVERWRITE_OR_IGNORE <overwrite_or_ignore_parameter>); COPY (WITH deduped AS (SELECT DISTINCT ON (id) <select_parameter> FROM '<s3://{{output of part 1}}/date={{date}}/**/*.parquet>' ORDER BY extracted_at DESC) SELECT <select_parameter> FROM deduped ORDER BY submission_date_time) TO '<s3://{{somewhere else}}/data_0.parquet>' (COMPRESSION <compression_parameter>, OVERWRITE_OR_IGNORE <overwrite_or_ignore_parameter>);",
    "applied_rules": [
        "<select_parameter>",
        "<date_parameter>",
        "<current_timestamp_parameter>",
        "<format_parameter>",
        "<compression_parameter>",
        "<overwrite_or_ignore_parameter>"
    ],
    "Affected Elements": "OutOfMemoryException, InvalidInputException",
    "Root Cause Analysis": "The root cause appears to be the inability of DuckDB to handle large data uploads in constrained memory environments, leading to incomplete writes and subsequently malformed parquet files."
}