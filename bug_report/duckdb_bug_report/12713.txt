ID: 12713
Title: Can't stream record batches from the same cursor that I'm using to insert rows
Description:
### What happens?

When I stream Arrow record batches from a query, I can't then use that same connection to insert rows into another table in the same database.

I **can** do that if I create a new cursor, but then I've lost access to all the temporary tables that I've registered on the top level connection.

### To Reproduce

```python
import duckdb
import pyarrow as pa

con = duckdb.connect()
con.execute("CREATE OR REPLACE TEMP TABLE source AS SELECT x FROM RANGE(100) _ (x)")
con.execute("CREATE OR REPLACE TABLE target(x INTEGER)")

n = 0

for batch in con.sql("FROM source").fetch_arrow_reader(batch_size=10):
    source_batch = pa.Table.from_batches([batch])
    con.execute("INSERT INTO target FROM source_batch")
    n += len(source_batch)

assert n == 100
```

```
    assert n == 100
           ^^^^^^^^
AssertionError
```

### OS:

x86_64-linux

### DuckDB Version:

1.0.0

### DuckDB Client:

Python

### Full Name:

Phillip Cloud

### Affiliation:

Voltron Data

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have