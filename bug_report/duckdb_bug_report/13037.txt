ID: 13037
Title: Memory leak in python when referencing a dataframe that is reused
Description:
### What happens?

Memory keeps growing when inserting into a duckdb table from a dataframe in a loop where the dataframe is reused to reference new data (i.e. it seems duckdb keeps a reference to the old object around).
When dealing with lots of data it ends crashing / failed to allocate.

Added checkpoint trying to see if it made a difference, doesn't seem like it.

From `memory_profiler` extract

```
Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    11    112.8 MiB    112.8 MiB           1   @profile
    12                                         def f():
    13    112.8 MiB      0.0 MiB           1       print("Removing existing db")
    14    112.8 MiB      0.0 MiB           1       Path("repro.duckdb").unlink()
    15    112.8 MiB      0.0 MiB         103       cols = [f"c{i}" for i in range(COLS)]
    16    114.4 MiB      1.6 MiB           1       conn = duckdb.connect("repro.duckdb")
    17                                         
    18    114.4 MiB      0.0 MiB           1       print("New slice")
    19    877.7 MiB    763.3 MiB           1       data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    20   2309.3 MiB   1431.6 MiB           1       conn.sql("create table t as select * from data_slice")
    21   2309.3 MiB      0.0 MiB           1       conn.checkpoint()
    22   2309.3 MiB      0.0 MiB           1       conn.commit()
    23                                         
    24   2309.3 MiB      0.0 MiB           1       print("New slice")
    25   2309.5 MiB      0.2 MiB           1       data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    26   2560.0 MiB    250.5 MiB           1       conn.sql("insert into t select * from data_slice")
    27   2560.0 MiB      0.0 MiB           1       conn.checkpoint()
    28   2560.0 MiB      0.0 MiB           1       conn.commit()
    29                                         
    30   2560.0 MiB      0.0 MiB           1       print("New slice")
    31   2560.0 MiB      0.0 MiB           1       data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    32   3383.8 MiB    823.8 MiB           1       conn.sql("insert into t select * from data_slice")
    33   3383.8 MiB      0.0 MiB           1       conn.checkpoint()
    34   3383.8 MiB      0.0 MiB           1       conn.commit()
    35                                         
    36   3383.8 MiB      0.0 MiB           1       print("New slice")
    37   3383.9 MiB      0.1 MiB           1       data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    38   3414.2 MiB     30.3 MiB           1       conn.sql("insert into t select * from data_slice")
    39   3414.2 MiB      0.0 MiB           1       conn.checkpoint()
    40   3414.2 MiB      0.0 MiB           1       conn.commit()
    41                                         
    42   3414.2 MiB      0.0 MiB           1       print("New slice")
    43   3414.3 MiB      0.1 MiB           1       data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    44   4356.8 MiB    942.5 MiB           1       conn.sql("insert into t select * from data_slice")
    45   4356.8 MiB      0.0 MiB           1       conn.checkpoint()
    46   4356.8 MiB      0.0 MiB           1       conn.commit()
```


### To Reproduce

Code is intentionally verbose to make is easy to spot

```
import duckdb
import numpy as np
import pandas as pd
from pathlib import Path
from memory_profiler import profile

COLS = 100
ROWS = 1000000


@profile
def f():
    print("Removing existing db")
    Path("repro.duckdb").unlink()
    cols = [f"c{i}" for i in range(COLS)]
    conn = duckdb.connect("repro.duckdb")

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("create table t as select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    print("New slice")
    data_slice = pd.DataFrame(np.random.randint(0, 100, (ROWS, COLS)), columns=cols)
    conn.sql("insert into t select * from data_slice")
    conn.checkpoint()
    conn.commit()

    conn.close()


if __name__ == "__main__":
    f()
```

### OS:

Linux

### DuckDB Version:

1.0, repro on latest too

### DuckDB Client:

Python

### Full Name:

Mauro Pagano

### Affiliation:

Independent consultant

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have