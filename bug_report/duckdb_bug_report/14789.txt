ID: 14789
Title: Memory leakage with fetch_arrow_reader() -> BatchRecordReader.read_next_batch()
Description:
### What happens?

I have been struggling with this error for a while now.

## The problem:

I have a large JSONL file (43GB once decompressed) that i'm converting into Parquet using DuckDB.
I need to perform additional post-processing, therefore I use Arrow.
The data being too large, I cannot transform the query into an Arrow table directly with .arrow(), thus I use `fetch_arrow_reader()`
However, whenever I try to iterate over the BatchRecord's, the process runs out of memory, even when reducing the first query.

Am I the only one to face this issue?

### To Reproduce

```python
import duckdb
import pyarrow

query = open("iss14789.sql", "r").read()
arrow_batches = duckdb.sql(query).fetch_arrow_reader(batch_size=1000)
```

```python
arrow_batches.read_next_batch()
> [1]    254276 segmentation fault (core dumped)
```

### OS:

wsl2

### DuckDB Version:

1.0.0

### DuckDB Client:

Python

### Hardware:

32GB RAM, i7

### Full Name:

Jérémy Arancio

### Affiliation:

Freelance

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have