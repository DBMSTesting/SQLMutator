ID: 16305
Title: OOM on complex join
Description:
### What happens?

I have loaded a number of parquet files into tables. Each has a `pid` column which is unique and joins the tables. I want to actually run the join, and loop over the outputs (to produce a huggingface `datasets` object) and I get oom.

I see on the docs "If multiple blocking operators appear in the same query, DuckDB may still throw an out-of-memory exception due to the complex interplay of these operators."

Is there anything I can do to get around this?

### To Reproduce

```
SET threads=1;
SET memory_limit='50GB';

SELECT seqs_and_envs.pid, protein_seq, structure_tokens, msa_string, neff, cumulative_weight, ogt, pHenv
FROM seqs_and_envs
INNER JOIN weights ON seqs_and_envs.pid = weights.pid
INNER JOIN struct_tokens ON seqs_and_envs.pid = struct_tokens.pid
INNER JOIN msa ON seqs_and_envs.pid = msa.pid
INNER JOIN msa_stats ON seqs_and_envs.pid = msa_stats.pid
LIMIT 10000
```

### OS:

LSB Version: :core-4.1-amd64:core-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.8 (Ootpa) Release: 8.8 Codename: Ootpa

### DuckDB Version:

1.2.0

### DuckDB Client:

Python

### Hardware:

I've tried on a couple of setups. Both Dual socket Intel Xeon Sapphire Rapids 52-core processors (104 cores total). In one case, 256 GB available DDR5 RAM (in which case the above mem limit was lower), and in another 2 TB available.

### Full Name:

Evan Komp

### Affiliation:

National Renewable Energy Lab

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have