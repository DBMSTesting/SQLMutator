ID: 14729
Title: Full table scan when extracting just a few records
Description:
### What happens?

DuckDB is really great!! But I'm facing a strange behaviour and I'm not sure if it's a bug or a feature...
I have a table with some 2-3 billion records (yes!).
If I just run a "select count(*) where blah and yaddah and blah", it runs fast and perfectly fine.
But if I do something like "select * where (same conditions)", surrounded by "COPY(...) to 'something.csv'", that's much slower. It looks like it's doing a full table scan _before_ applying the conditions, not _after_.
Am I doing something wrong?
Thanks!!

### To Reproduce

Fast, normal behaviour:

`select count(*) from table_with_millions_of_records where
field1 = 2 and
sqrt(field2**2 + field3**2) and
field4 > 5 and
field5/field6 > 10 and
field7 > 20`

Slow, full-table-scan behaviour:

`COPY( select * from table_with_millions_of_records where
field1 = 2 and
sqrt(field2**2 + field3**2) and
field4 > 5 and
field5/field6 > 10 and
field7 > 20 )
TO 'MyExtractedRecords.csv'`

### OS:

Alma Linux 9

### DuckDB Version:

v1.1.2 f680b7d08f

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Jordi

### Affiliation:

Univ. Barcelona

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have