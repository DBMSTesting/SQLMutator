ID: 12941
Title: Copying constant aggregate / window to every row of intermediate results
Description:
### What happens?

A query to compute median over the entire result set (plus some other bits and pieces) ultimately caused the process to crash due to exhausting memory. 

We ultimately can work around this with a more reasonable query, but @hawkfish asked me to file this as a performance issue: https://discord.com/channels/909674491309850675/1260698841733136466/1260709418383708212

> This is not technically a bug but the real issue is that there is only one copy of the aggregate so we can optimise it as a constant vector instead of copying it 2048 times .Please file this as a perf issue.

His explanation of why it was ballooning memory: 
> So basically you are constructing a 100K element list using windowing,  copying it to every row and then independently computing the median on each row. Still that seems like a lot of RAM for that.
> Maybe not. 2K copies of 800K bytes = 1.6GB on 6 threads (my machine) => 9.6GB.

Ideally, I could limit the amount of memory used by a query (or thread) for a query as apparently this memory doesn't count towards the memory_limit parameter, but it doesn't sound like that's an option. 

As reported by activity monitor on my mac, with 10 threads this query spikes duckdb memory usage up to 30GB. And limiting the memory used by duckdb with the `memory_limit` has no effect.

### To Reproduce

Simple example: 
```
with baseTable as (
  SELECT i.generate_series num 
  FROM generate_series(1, 100000) i
) 
select 
  num, 
  LIST_AGGREGATE(FLATTEN(LIST_VALUE(ARRAY_AGG(num) over ())), 'MEDIAN')
from baseTable limit 10;
```

### OS:

mac aarch64, ubuntu x86

### DuckDB Version:

1.0.0

### DuckDB Client:

cli, java

### Full Name:

Jonathan Swenson

### Affiliation:

Omni

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have