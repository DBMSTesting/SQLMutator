ID: 14434
Title: No convenient way to read CSV files generated by MySQL
Description:
### What happens?

Hi DuckDB team,

I found that there is no convenient way to read the CSV file generated by MySQL when the string values contain special characters like `\t`, `\n`, and `\\`.

MySQL uses `\` [by default](https://dev.mysql.com/doc/refman/8.4/en/load-data.html#load-data-field-line-handling) as the escape character, and it will escape the special characters in the file. It seems DuckDB cannot handle this case due to the meaning of the `escape` parameter in the `read_csv` function is different from the `FIELDS ESCAPED BY` parameter in MySQL.

I think it would be great if DuckDB can read such files, as it is a common case when exporting data from MySQL.


### To Reproduce

The following is the example to reproduce the issue.

```sh
docker run -d --name csv-test \
  -e MYSQL_ROOT_PASSWORD=root \
  -p 13316:3306 \
  mysql:latest \
  mysqld --secure-file-priv=/tmp

mysql -h127.0.0.1 -uroot -proot -P13316 <<EOF
create database db01;
use db01;
create table t (i int, s varchar(32), j int);
insert into t values (1, '\t', 1), (2, '\n', 2), (3, 'a\ta', 3), (4, 'b\nb', 4), (5, 'c\\c', 5);
table t into outfile '/tmp/t.txt';
EOF

docker cp csv-test:/tmp/t.txt ./t.txt

duckdb -c "select * from read_csv('./t.txt', sep = '\t', quote = '', escape = '\', header = false)"
```

```
Invalid Input Error: Error when sniffing file "./t.txt".
It was not possible to automatically detect the CSV Parsing dialect/types
The search space used was:
Delimiter Candidates: '	'
Quote/Escape Candidates: ['\0','"'],['\0','\0'],['\0',''']
Comment Candidates: '#', '\0'
Possible fixes:
* Delimiter is set to '	'. Consider unsetting it.
* Quote is set to '\0'. Consider unsetting it.
* Escape is set to '\'. Consider unsetting it.
* Set comment (e.g., comment='#')
* Set skip (skip=${n}) to skip ${n} lines at the top of the file
* Enable ignore errors (ignore_errors=true) to ignore potential errors
* Enable null padding (null_padding=true) to pad missing columns with NULL values
* Check you are using the correct file compression, otherwise set it (e.g., compression = 'zstd')
```

### OS:

macOS aarch64

### DuckDB Version:

v1.1.1

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Fan Yang

### Affiliation:

ApeCloud

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have