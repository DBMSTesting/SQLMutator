ID: 14315
Title: Adding s3 parameters to `COPY .. TO .. s3` cause errors when `file_size_bytes` is used
Description:
### What happens?

Adding s3 parameters to `COPY .. TO .. s3` cause errors when `file_size_bytes ` is also used. Other fields are not relevant.

### To Reproduce

This works fine:
```sql
 COPY (SELECT 1 id, 2 id2, 3 id3) TO 's3://mytestdatasets/temp/data.parquet' WITH (file_size_bytes '512MB');
```

The same command fails when I add `?s3_region=us-east-1`. The new file doesn't exists, but DuckDB complains that it exists:
```sql
 COPY (SELECT 1 id, 2 id2, 3 id3) TO 's3://mytestdatasets/temp/data_2.parquet?s3_region=us-east-1' WITH (file_size_bytes '512MB');
IO Error: Directory "s3://mytestdatasets/temp/data_3.parquet?s3_region=us-east-1" is not empty! Enable OVERWRITE option to overwrite files
```


Let me add overwrite_or_ignore as DuckDB requests, still fails:

```sql
COPY (SELECT 1 id, 2 id2, 3 id3) TO 's3://mytestdatasets/temp/data_2.parquet?s3_region=us-east-1' WITH (file_size_bytes '512MB', overwrite_or_ignore);
HTTP Error: Unable to connect to URL "https://mytestdatasets.s3.us-east-1.amazonaws.com/temp/data_2.parquet": 400 (Bad Request)
```


In bash, verify that the region is us-east-1
```sh
curl -sI https://mytestdatasets.s3.amazonaws.com/ | grep bucket-region 
x-amz-bucket-region: us-east-1
```


### OS:

Both MacOs, and Ubuntu 22.04.4 LTS on WSL2

### DuckDB Version:

1.1.1

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Onder KALACI

### Affiliation:

  CrunchyData

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have not tested with any build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have