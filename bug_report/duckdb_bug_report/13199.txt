ID: 13199
Title: Inserting mass data into a table kills the process after a while using java / linux
Description:
### What happens?

Add data to a table with this structure using the Appender-class

ts	BIGINT
ts_year	INTEGER
ts_month	INTEGER
ts_day	INTEGER
ts_value	DOUBLE
ts_status	BIGINT
market_id	VARCHAR
parameter	VARCHAR
subnet	VARCHAR
net	VARCHAR
adrcity	VARCHAR
adrstreet	VARCHAR
metersize	VARCHAR
connectobj	VARCHAR
tecstatus	VARCHAR
edl	VARCHAR
watheatcontyp	VARCHAR
regulvalve	VARCHAR
heattype	VARCHAR
loadprofile	VARCHAR
ventilation	BOOLEAN
solar	BOOLEAN
waterheating	BOOLEAN
planttype	VARCHAR
tariftype	VARCHAR
readingdev	VARCHAR
waterheattype	VARCHAR

all attributes nullable, no special other properties

kills the process after a while. Restarting the process I can resume the process, but it kills again after a while. 
I tried to append the data in portions: commit after each portion, close the db-connection, open the db-connection. But still experiencing this effect: the process will be killed after some (3-4) portions, each portion having several 100000 values. 


Using java11 openjdk 11.0.21 2023-10-17 on Linux.

Linux is:
uname -a
Linux  6.1.0-23-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.99-1 (2024-07-15) x86_64 GNU/Linux


I tried DuckDB 1.0.0, and older version 0.9.0 and a 0.8.x version

This is the memory situation in that vm:
               total        used        free      shared  buff/cache   available
Mem:         8131820     3284572     2857412      139356     2412984     4847248
Swap:        1003516      490460      513056

In another project where we still use an 0.8.x version there is a similar task running without problems (windows, 64MB main memory)
I assume it is a memory allocation problem, but the task kills without any further information (no exception, or something similar). It just stops without any message or information

The expected behaviour is actually in this case that the process can run and terminate successfully, since we work in portions. 
There is no workaround available for me (even close and open the connection seems not to reset the duckdb status completeley). I must restart the process to continue. 

I have much data I could provide somehow in a zipped - csv file, for example, which might help to replicate the problem.

In order to avoid te Appender.addNull-problem, I have extended the appender to accept null values similar to the ticket I saw here. 

### To Reproduce

// Read data from csv or any other source
// open connection
// use appender to add the data (several 100.000 records, see structure above)
// commit 
// cloase connection

// repeat the steps above. 
// Process will kill after several times without message

### OS:

Linux 

### DuckDB Version:

1.0.0 and older versions

### DuckDB Client:

java

### Full Name:

Burkhard Losch

### Affiliation:

KISTERS AG

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [ ] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have