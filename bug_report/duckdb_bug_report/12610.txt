ID: 12610
Title: Python API suggests increasing maximum_object_size when read_json but it's unclear how to do that
Description:
### What happens?

When I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.

Code snippet / output:
```
In [3]: import duckdb

In [4]: db = duckdb.connect("foo.db")

In [5]: db.read_json("oldbytes.space.user.feoh.json")
---------------------------------------------------------------------------
InvalidInputException                     Traceback (most recent call last)
Cell In[5], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json")

InvalidInputException: Invalid Input Error: "maximum_object_size" of 16777216 bytes exceeded while reading file "oldbytes.space.user.feoh.json" (>33554428 bytes).
 Try increasing "maximum_object_size".

In [6]:
```

Trying to pass that param as part of the reas_json call:
```
In [7]: db.read_json("oldbytes.space.user.feoh.json", maximum_object_size=6000000000)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json", maximum_object_size=6000000000)

TypeError: read_json(): incompatible function arguments. The following argument types are supported:
    1. (self: duckdb.duckdb.DuckDBPyConnection, name: str, *, columns: typing.Optional[object] = None, sample_size: typing.Optional[object] = None, maximum_depth: typing.Optional[object] = None, records: typing.Optional[str] = None, format: typing.Optional[str] = None) -> duckdb.duckdb.DuckDBPyRelation

Invoked with: <duckdb.duckdb.DuckDBPyConnection object at 0x104e476b0>, 'oldbytes.space.user.feoh.json'; kwargs: maximum_object_size=6000000000
```




### To Reproduce

Run the following code with a very large JSON file:
When I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.

Code snippet / output:
```
In [3]: import duckdb

In [4]: db = duckdb.connect("foo.db")

In [5]: db.read_json("oldbytes.space.user.feoh.json")
---------------------------------------------------------------------------
InvalidInputException                     Traceback (most recent call last)
Cell In[5], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json")

InvalidInputException: Invalid Input Error: "maximum_object_size" of 16777216 bytes exceeded while reading file "oldbytes.space.user.feoh.json" (>33554428 bytes).
 Try increasing "maximum_object_size".

In [6]:
```



### OS:

MacOS Sonoma 14.5, Ubuntu 24.03

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

Python

### Full Name:

Christopher Patti

### Affiliation:

MIT Online Learning

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have