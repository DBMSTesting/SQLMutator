ID: 13787
Title: CSV import swallows some lines for large CSVs.
Description:
### What happens?

The CSV importer seems to ignore some rows for large files.

After importing the [Clickbench](https://github.com/ClickHouse/ClickBench) data set, DuckDB reports 99992000 rows, while Postgres and `wc -l` correctly report 99997497 rows.

### To Reproduce

1. Download and unpack the hits.tsv file from Clickhouse: https://datasets.clickhouse.com/hits_compatible/hits.tsv.gz
   ```bash
   wget https://datasets.clickhouse.com/hits_compatible/hits.tsv.gz
   ```
2. Create the corresponding schema in a new duckdb database: https://github.com/ClickHouse/ClickBench/blob/main/duckdb/create.sql
3. Load the CSV:
   ```sql
   COPY hits FROM 'hits.tsv.gz';
   ```
4. Execute the following query:
    ```sql
    SELECT count(*) FROM hits;
    ```
    It (incorrectly) returns 99992000.

### OS:

Ubuntu 24.04 on x86_64

### DuckDB Version:

v1.0.1-dev5221 53d4c90b8a

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Lukas Vogel

### Affiliation:

CedarDB

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have