ID: 12805
Title: Can't shard + index 0.5P of data
Description:
### What happens?

Tried to index 0.5P of data for @onefact (with support from @aws via a grant and @google via a TPU grant), but cannot due to SSD/disk drive write speeds and sharding strategies, combined with the inability of @duckdb with experimental support for vector search at that scale. 

(@Maxxen 's excellent work on helping us fix 300k vectors will be great to get working but insufficient for our scale: https://github.com/duckdb/duckdb_vss/issues/19)

cc @mansimov who has been helping us on the foundation model front from @aws and can add more folks from @google's https://github.com/google/jax team helping us out with TPU stuff for sharding strategies.

@google's jax and @duckdb are the few tools able to scale to this size, and I would love to make a nice demo for the DuckCon üê£. HOwever I am scared I will be unable to do it without additional help from the community. COntributions & repros welcome! Thank you for any advice and input.

### To Reproduce

Index 300k or more vectors, randomly from 0.5P of data (it's ok if the data is randomly generated, we can backfill with our health care and healthcare-related data for the live demo).

### OS:

Ubuntu 22.04 LTS (AWS) or https://cloud.google.com/blog/topics/hpc/salk-institute-brain-mapping-on-google-cloud-with-skypilot for TPUs

### DuckDB Version:

Latest

### DuckDB Client:

Python

### Full Name:

Dr. Jaan Lƒ±

### Affiliation:

University of Tartu & @onefact

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have