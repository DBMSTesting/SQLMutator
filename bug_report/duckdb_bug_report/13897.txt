ID: 13897
Title: Writing to hive partitioned parquet dataset results in invalid parquet dataset.
Description:
### What happens?

Using   

```
COPY ( 
   SELECT 
       UNNEST(results,  max_depth := 2) 
   FROM 
       read_ndjson('data/json/*.json')
) TO 'data/data' ",
    (FORMAT PARQUET, COMPRESSION SNAPPY, PARTITION_BY 'publication_year')"
) 
```

results in an invalid parquet dataset which can not be opened as here seen from R:

```
arrow::open_dataset("data/data")
Error in `arrow::open_dataset()`:
! Type error: Unable to merge: Field publication_year has incompatible types: int64 vs int32
Show Traceback

> arrow::open_dataset("data/data", unify_schemas = TRUE)
Error in `arrow::open_dataset()`:
! Type error: Unable to merge: Field publication_year has incompatible types: int64 vs int32
Show Traceback
```

The individual datasets can be found for the json files at https://github.com/rkrug/openalexr_json/tree/b67560708abf711fdc0e5c9b26c2327e12b7cc1b/data/json

and for the resulting hive partitioned parquet dataset at https://github.com/rkrug/openalexr_json/tree/b67560708abf711fdc0e5c9b26c2327e12b7cc1b/data/data

When using `DESCRIBE` for the json dataset, it describes `publication_year` as `BGINT`, and exporting to a single parquet file also works.

### To Reproduce

```
json_to_parquet <- function(
    json_dir = file.path("data", "json"),
    arrow_dir = file.path("data", "data")) {
  ## Define set of json files

  ## Create in memory DuckDB
  con <- DBI::dbConnect(duckdb::duckdb())

  on.exit(
    DBI::dbDisconnect(con, shutdown = TRUE)
  )

  ## Install and load jsonq
  paste0(
    "INSTALL json"
  ) |>
    DBI::dbExecute(conn = con)

  paste0(
    "LOAD json"
  ) |>
    DBI::dbExecute(conn = con)

  paste0(
    "COPY ( ",
    "   SELECT ",
    "       UNNEST(results,  max_depth := 2) ",
    "   FROM ",
    "       read_ndjson('", json_dir, "/*.json', maximum_object_size=1000000000)",
    ") TO '", arrow_dir, "' ",
    "(FORMAT PARQUET, COMPRESSION SNAPPY, PARTITION_BY 'publication_year')"
  ) |>
    DBI::dbExecute(conn = con)
}
```

This is the function to export the json files to the hive partitioned parquet dataset. Both the json files and the resulting hive partitioned parquet dataset are available via GitHub:

json: https://github.com/rkrug/openalexr_json/tree/b67560708abf711fdc0e5c9b26c2327e12b7cc1b/data/json

hive partitioned parquet dataset: https://github.com/rkrug/openalexr_json/tree/b67560708abf711fdc0e5c9b26c2327e12b7cc1b/data/data



### OS:

macOS

### DuckDB Version:

v1.1.0

### DuckDB Client:

R

### Hardware:

_No response_

### Full Name:

Rainer M Krig

### Affiliation:

University of ZÃ¼rich

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have not tested with any build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have