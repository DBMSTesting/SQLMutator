ID: 14526
Title: [PySpark] Test Spark API with actual PySpark as backend
Description:
Following-up on [this comment](https://github.com/duckdb/duckdb/pull/14458#issuecomment-2426124842) from @Tishj. 

Approach:
* By setting the `USE_ACTUAL_SPARK` env variable to `true`, one can now run all Spark API tests against an actual PySpark backend.
  * E.g. `USE_ACTUAL_SPARK=true python -m pytest tests/fast/spark`
  * For local development, this would require Java and Spark to be installed
* I've also set this up as part of the `Python 3.9 Linux` workflow job so it runs on every pull request. I think with this, it's also fine that not every developer will run it against Spark in production as they can use the CI for it.
  * You can see that it uses Spark in CI as the Spark tests take >40s to complete... With DuckDB, it's around 2s ;) Locally, you can also add the `-s` argument to Pytest which captures all output and which shows some PySpark output.
* Wherever you see `USE_ACTUAL_SPARK` in the tests, it means that there is a difference between DuckDB and Spark.
  * It's not that much which is very nice! I think some of the differences are ok and with this, it should be easy to find them and to make a conscious decision if they should be fixed or not.

Some thoughts on why I went with a `spark_namespace` package:
* As @Tishj, I've also tried to overwrite the Python import system to either use PySpark or DuckDB based on a Pytest command-line argument. I did not manage to make this reliable enough so i works for all cases and won't easily break in the future.
* An alternative would have been a pytest fixture which provides this namespace. It's a reliable way but it makes the tests more verbose as we canâ€™t just import e.g. `Row` once but have to extract it every time from the namespace provided by the fixture
* Having this separate package which abstracts away the logic allowed for only minor changes to existing code and it's reliable. As long as we always import from there, it should not happen that the wrong package is used.

Main changes to tests:
* If something is read from file, before comparing it, we need to order the rows
* `assert "column" in df` does not work with PySpark and needs to be `assert "column" in df.columns`
* imports

I chose feature as target branch as it already contains some relevant changes from other PRs