ID: 15174
Title: OOM on ORDER BY query
Description:
### What happens?

While sorting a table with 200M double entries (=1.6GB of uncompressed data) and writing the results back to parquet, DuckDB uses more than the `MEMORY_LIMIT` and gets out-of-memory killed.

### To Reproduce

In a plain `./duckdb` terminal instance, run 
```sql
SET memory_limit='9GB';
CREATE OR REPLACE TABLE df AS (SELECT random() AS x FROM range(200_000_000));
COPY (SELECT * FROM df ORDER BY x) TO '/tmp/test.parquet';
```
```
 45% ▕███████████████████████████                                 ▏ Killed
```

At the time of the OOM-kill, DuckDB uses >10.5GB of memory according to htop-RES. 



### OS:

Ubuntu 20.04, x86_64

### DuckDB Version:

1.1.3

### DuckDB Client:

Terminal

### Hardware:

16GB RAM

### Full Name:

Soeren Wolfers

### Affiliation:

G-Research

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have