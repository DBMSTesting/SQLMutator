ID: 14796
Title: Memory leakage with fetch_arrow_reader() -> BatchRecordReader.read_next_batch()
Description:
I have been struggling with this error for a while now.

## The problem:

I have a large JSONL file (43GB once decompressed) that i'm converting into Parquet using DuckDB.
I need to perform additional post-processing, therefore I use Arrow.
The data being too large, I cannot transform the query into an Arrow table directly with .arrow(), thus I use `fetch_arrow_reader()`
However, whenever I try to iterate over the BatchRecord's, the process runs out of memory, even when reducing the first query.

Am I the only one to face this issue?

## Code

```python
import duckdb
query = sql_query_file.read_text().replace("{dataset_path}", dataset_path)
arrow_batches = duckdb.sql(query).fetch_arrow_reader(batch_size=1000)
```

```python
arrow_batches.read_next_batch()
> [1]    254276 segmentation fault (core dumped)
```