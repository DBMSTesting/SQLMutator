ID: 12856
Title: `read_csv()` fails to recognize DOUBLE fields (uses VARCHAR instead) when `decimal_separator` is set, and there are more than 2047 lines in the CSV file
Description:
### What happens?

`read_csv()` fails to recognize DOUBLE fields (uses VARCHAR instead) when `decimal_separator` is set, and there are more than 2047 lines in the CSV file.

### To Reproduce


## 1. This works: CSV file with 2047 lines

Create a CSV file with 2047 lines:
```
echo "text|number" >2047.csv; for a in {1..2047}; do echo "foo|123,000" >>2047.csv; done
```

This creates a CSV file with the content:

```
text|number
foo|123,000
foo|123,000
foo|123,000
foo|123,000
foo|123,000
foo|123,000
...
```

Run `read_csv` on it:
```sql
SUMMARIZE SELECT * FROM read_csv('2047.csv', decimal_separator=',');
```
```
┌─────────────┬─────────────┬─────────┬─────────┬───────────────┬─────────┬─────────┬─────────┬─────────┬─────────┬───────┬─────────────────┐
│ column_name │ column_type │   min   │   max   │ approx_unique │   avg   │   std   │   q25   │   q50   │   q75   │ count │ null_percentage │
│   varchar   │   varchar   │ varchar │ varchar │     int64     │ varchar │ varchar │ varchar │ varchar │ varchar │ int64 │  decimal(9,2)   │
├─────────────┼─────────────┼─────────┼─────────┼───────────────┼─────────┼─────────┼─────────┼─────────┼─────────┼───────┼─────────────────┤
│ text        │ VARCHAR     │ foo     │ foo     │             1 │         │         │         │         │         │  2047 │            0.00 │
│ number      │ DOUBLE      │ 123.0   │ 123.0   │             1 │ 123.0   │ 0.0     │ 123.0   │ 123.0   │ 123.0   │  2047 │            0.00 │
└─────────────┴─────────────┴─────────┴─────────┴───────────────┴─────────┴─────────┴─────────┴─────────┴─────────┴───────┴─────────────────┘
```

The `number` field gets recognized correctly as `DOUBLE`.

BUT:

## 2. This bugs: CSV file with 2048 lines

Same with 2048 lines, create the CSV file:
```
echo "text|number" >2048.csv; for a in {1..2048}; do echo "foo|123,000" >>2048.csv; done
```

Run read_csv:

```sql
SUMMARIZE SELECT * FROM read_csv('2048.csv', decimal_separator=',');
```
```
┌─────────────┬─────────────┬─────────┬─────────┬───────────────┬───────┬───────┬───────┬───────┬───────┬───────┬─────────────────┐
│ column_name │ column_type │   min   │   max   │ approx_unique │  avg  │  std  │  q25  │  q50  │  q75  │ count │ null_percentage │
│   varchar   │   varchar   │ varchar │ varchar │     int64     │ int32 │ int32 │ int32 │ int32 │ int32 │ int64 │  decimal(9,2)   │
├─────────────┼─────────────┼─────────┼─────────┼───────────────┼───────┼───────┼───────┼───────┼───────┼───────┼─────────────────┤
│ text        │ VARCHAR     │ foo     │ foo     │             1 │       │       │       │       │       │  2048 │            0.00 │
│ number      │ VARCHAR     │ 123,000 │ 123,000 │             1 │       │       │       │       │       │  2048 │            0.00 │
└─────────────┴─────────────┴─────────┴─────────┴───────────────┴───────┴───────┴───────┴───────┴───────┴───────┴─────────────────┘
```

As you can see, here `| number      │ VARCHAR     │ 123,000 │ 123,000` is not recognized as `DOUBLE`, but `VARCHAR` gets used instead.

### OS:

Linux, Windows

### DuckDB Version:

v1.0.1-dev2164

### DuckDB Client:

CLI

### Full Name:

K Kovacs

### Affiliation:

Priority Queue Consulting

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have