ID: 14898
Title: ROW_GROUP_SIZE parameter doesn't work for some parquet files
Description:
### What happens?

When using `COPY ... TO ...` to generate a new parquet file from another parquet file the `ROW_GROUP_SIZE` parameter doesn't work.

The final row group size is very low (under 10 000)

### To Reproduce

Parquet file to reproduce is available here : https://drive.google.com/file/d/1qKeMmM1UmE8KlmYEilVzQghl8l_R64Pu/view?usp=drive_link
It's generated from [this SAS file](https://www2.census.gov/programs-surveys/ahs/2019/AHS%202019%20National%20PUF%20v1.1%20Flat%20SAS.zip) with [readstat](https://github.com/curtisalexander/readstat-rs)

I have the same problem with every parquet generated by readstat

The SQL code to reproduce it : 

```sql
SET preserve_insertion_order = false;

SELECT * FROM duckdb_settings() WHERE name = 'memory_limit';

SELECT MAX(row_group_num_rows) FROM parquet_metadata('ahs.parquet');

COPY (FROM read_parquet('ahs.parquet')) TO 'mynewfile.parquet' (FORMAT PARQUET, ROW_GROUP_SIZE 122_880);

SELECT COUNT(*) FROM read_parquet('mynewfile.parquet');

SELECT MAX(row_group_num_rows) FROM parquet_metadata('mynewfile.parquet');
```

Output is : 

```
duckdb-bleeding < reprex.sql
┌──────────────┬───────────┬─────────────────────────────────────────────┬────────────┬─────────┐
│     name     │   value   │                 description                 │ input_type │  scope  │
│   varchar    │  varchar  │                   varchar                   │  varchar   │ varchar │
├──────────────┼───────────┼─────────────────────────────────────────────┼────────────┼─────────┤
│ memory_limit │ 403.0 GiB │ The maximum memory of the system (e.g. 1GB) │ VARCHAR    │ GLOBAL  │
└──────────────┴───────────┴─────────────────────────────────────────────┴────────────┴─────────┘
┌─────────────────────────┐
│ max(row_group_num_rows) │
│          int64          │
├─────────────────────────┤
│                   10000 │
└─────────────────────────┘
100% ▕████████████████████████████████████████████████████████████▏
┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│        63185 │
└──────────────┘
┌─────────────────────────┐
│ max(row_group_num_rows) │
│          int64          │
├─────────────────────────┤
│                    4096 │
└─────────────────────────┘
```


### OS:

ubuntu 22.04 x86_64 

### DuckDB Version:

1.1.2, 1.1.3 and bleeding

### DuckDB Client:

cli and R

### Hardware:

_No response_

### Full Name:

Nicolas chuche

### Affiliation:

Ministere de la transition écologique

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have