ID: 13177
Title: Seg fault loading a 240gb CSV file with large text columns
Description:
### What happens?

load of large CSV files with big text columns fails with a segv




### To Reproduce

here is a standalone script. unfortunately, its a very big file.
[segv.sh.txt](https://github.com/user-attachments/files/16405058/segv.sh.txt)


### OS:

MacOS X Apple M2 Version 14.5 (23F79)

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

CLI

### Full Name:

Bob Pasker

### Affiliation:

none

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have