ID: 16157
Title: v1.2.0 partitioned write throws basic_string::_S_construct null not valid
Description:
### What happens?

I'm trying to create parquet files from a larger parquet file generated from Bigquery with hive partitioned write using `partition_by` argument in copy ... into ..., DuckDB throws `duckdb.duckdb.Error: basic_string::_S_construct null not valid`. It worked fine with DuckDB v1.1.3.

The data set is not included as the attachment doesn't support parquet files. However, it can simply generated by running the following SQL in Bigquery
```
EXPORT DATA
OPTIONS (
    URI=<any GCS URI>
    FORMAT='PARQUET',
    COMPRESSION='ZSTD',
    OVERWRITE=TRUE
) AS (
    SELECT
      "123" as id,
      current_date as date
)
```
Worth to notice is that running the `copy into` statement without `partition_by` argument works.

### To Reproduce

```sql
COPY (
    SELECT * FROM read_parquet('test_input.parquet')
)
TO 'test'
(
    FORMAT 'parquet',
    PARTITION_BY id
);
```

### OS:

Windows 11 with WSL2

### DuckDB Version:

1.2.0

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

L. Jiacheng

### Affiliation:

none

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have