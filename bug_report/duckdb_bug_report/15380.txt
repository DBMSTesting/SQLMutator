ID: 15380
Title: provide ability to specify schema while reading parquet
Description:
### What happens?

(Followup of #15375)

This is an enhancement request as we are evaluating DuckDB to replace existing engine in production.  
In my environment Terabytes of data is stored in partitioned parquet files and we analyze the dataset partition by partition. 
However some of the partitions are old and do not have few columns as those columns were added later.
While running sql on those old partitions even when  `union_by_name = true` we run into error
```
Binder Error: Referenced column "a" not found in FROM clause!
Candidate bindings: "read_parquet.s"
LINE 1: select a from read_parquet('2.parquet') ;
```
Other engine such as spark let you specify the schema in the query itself so that they can be replaced with `null` even though those columns are not present in any of those files.
By providing the ability to specify the schema the performance penalty of `union_by_name` parameter can also be avoided since schema resolution is not required.


### To Reproduce

```sql
copy ( select {'i' : 3, 'j' : 99 } as s ) TO '2.parquet' (FORMAT 'parquet', COMPRESSION 'zstd', ROW_GROUP_SIZE 100_000);
select a from read_parquet('2.parquet') ;
```

### OS:

All

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

command line

### Hardware:

_No response_

### Full Name:

Gagan Taneja

### Affiliation:

self 

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have