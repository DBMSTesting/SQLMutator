ID: 15816
Title: Inconsistent JSON Behavior: INSERT INTO SELECT is case insensitive with keys; COPY is not
Description:
### What happens?

When you COPY from a JSON file, your query fails if your table's column names do not match the case of your file's column names.  However, when you INSERT INTO SELECT from a JSON file, your query succeeds even if the cases don't match.

Impact: This inconsistency makes it difficult to write reliable data pipelines, as the same JSON file might work with one command but fail with another.

Expected Behavior:
1. Both COPY and INSERT INTO SELECT should be case insensitive with JSON keys (**preferred**)
2. Both should be case sensitive (**not preferred** as this does not match other databases and will break my pipelines)
3. There should be an option to specify case sensitivity

### To Reproduce

To reproduce use these json files:
[device_metadata_uppercase_keys.json](https://github.com/user-attachments/files/18482380/device_metadata_uppercase_keys.json)
[device_metadata_lowercase_keys.json](https://github.com/user-attachments/files/18482379/device_metadata_lowercase_keys.json)
These files are identical except for the case of the keys.

Then run the following code:
```
CREATE OR REPLACE TABLE device_metadata (
    device_id VARCHAR,
    device_name VARCHAR
);

-- Working Cases
INSERT INTO device_metadata SELECT * FROM 'device_metadata_lowercase_keys.json';
INSERT INTO device_metadata SELECT * FROM 'device_metadata_uppercase_keys.json';
COPY device_metadata FROM 'device_metadata_lowercase_keys.json' WITH (FORMAT JSON, auto_detect TRUE);

-- Failed Case
COPY device_metadata FROM 'device_metadata_uppercase_keys.json' WITH (FORMAT JSON, auto_detect TRUE);
```

Error Message
```
SQL Error: Invalid Input Error: JSON transform error in file "/Users/ryanwaldorf/dev/duckdb_tests/device_metadata_uppercase_keys.json", in line 1: Object {"DEVICE_ID":"DEV000","DEVICE_NAME":"Motor_000"} has unknown key "DEVICE_ID"
Try increasing 'sample_size', reducing 'maximum_depth', specifying 'columns', 'format' or 'records' manually, setting 'ignore_errors' to true, or setting 'union_by_name' to true when reading multiple files with a different structure.
```


### OS:

MacOS 15.1.1 (24B91)

### DuckDB Version:

1.1.3

### DuckDB Client:

dbeaver

### Hardware:

_No response_

### Full Name:

Ryan Waldorf

### Affiliation:

Universql

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have