ID: 14735
Title: Setting temp_directory to a different drive gets completely ignored in Python
Description:
### What happens?

Working with Python Jupyter Notebooks in VSCode, when I create an in-memory database and set temp_directory to be a folder on another drive (not C: which it is by default) and run a query on a Polars DataFrame with larger-than-memory result, the query completely ignores temp_directory and continues spilling on C:.

### To Reproduce

1. Create a Jupyter Notebook file and open it in VSCode
2. 
```
import duckdb
import polars as pl

conn = duckdb.connect(':memory:') #same problem also happens when specifying :default: or nothing at all
conn.sql("SET temp_directory = 'E:/Coding/temp_dir'") #my path, obviously can be any other path not on default drive
```
3. run any very big SELECT query FROM polars_df (not sure if Polars matters but this is what I'm trying to do) that won't fit into memory, e.g.
`conn.sql('SELECT some_huge_transformations from polars_df') #also happens with conn.execute()` 
4. Observe disk space while the query is running, in my case it was disk C: that rapidly started filling up.


### OS:

Windows 10 x64

### DuckDB Version:

1.1.0

### DuckDB Client:

Python 3.10.7 through Jupyter Notebooks in VSCode

### Hardware:

_No response_

### Full Name:

Yaroslav Shelestov

### Affiliation:

Not affiliated (trying to use DuckDB for my own pet project)

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have