ID: 14095
Title: Slow fetch_df() on S3 remote query
Description:
### What happens?

Hello there,

we have a on-prem k8s cluster, with a hive partitioned Minio based bucket containing for test purposes ~ 50 files with total less than 50 MB.

We constructed these three queries:

```python
first = duckdb.query("SELECT * FROM read_parquet('s3://first-data/latest/*/*/*/*.parquet', filename=True);")
second = duckdb.query("SELECT * FROM read_parquet('s3://second-data/latest/*/*/*/*.parquet', filename=True);")
df = duckdb.sql("SELECT * FROM first JOIN second ON first.A = second.A and parse_filename(first.filename)=parse_filename(second.filename)")

df.fetchdf() <-- this one is slow
```
This query takes round about ~120 seconds until the ~ 850 records are reported

if we change the construction to

```python
first = duckdb.query("SELECT * FROM read_parquet('s3://first-data/latest/*/*/*/*.parquet', filename=True);").fetchdf()
second = duckdb.query("SELECT * FROM read_parquet('s3://second-data/latest/*/*/*/*.parquet', filename=True);").fetchdf()
df = duckdb.sql("SELECT * FROM first JOIN second ON first.A = second.A and parse_filename(first.filename)=parse_filename(second.filename)")

df.fetchdf() 
```
its absolute within reasonable expectation and we are super happy (something around 5 - 8 seconds which is in line with our network capabilities) 

The first parquet we query has about 1750 columns, the second has about 20 columns. The threads as been SET to 32 `duckdb.query("SET threads = 32;")`


Most certainly we do something wrong or we found some kind of bug or it is intended behavior, if so can somebody shortly explain the lengthy runtime in comparison?
 
Can somebody give us a helping hand here? 

Many thanks in advance!


### To Reproduce

``` python
import duckdb 
import time

start = time.perf_counter()

AWS_ACCESS_KEY_ID = "XXXXXXXXXXx"
AWS_SECRET_ACCESS_KEY = "XXXXXXXXX"
endpoint = "XXXXXXXXXXXXXXX"

duckdb.query("INSTALL httpfs; LOAD httpfs;")
duckdb.query("INSTALL parquet; LOAD parquet;")

duckdb.query(f"SET s3_access_key_id='{AWS_ACCESS_KEY_ID}';")
duckdb.query(f"SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';")
duckdb.query(f"SET s3_endpoint='{endpoint}';")
duckdb.query(f"SET s3_use_ssl='false';")
duckdb.query(f"SET s3_url_style='path';")
duckdb.query("SET threads = 32;")

end = time.perf_counter()
print(end-start)


start = time.perf_counter()

first = duckdb.query("SELECT * FROM read_parquet('s3://first/latest/*/*/*/*.parquet', filename=True);").fetchdf()
end = time.perf_counter()
print(end-start)
start = time.perf_counter()
second = duckdb.query("SELECT * FROM read_parquet('s3://seconds/latest/*/*/*/*.parquet', filename=True);").fetchdf()
end = time.perf_counter()
print(end-start)

start = time.perf_counter()
df = duckdb.sql("SELECT * FROM first JOIN secodn ON first.A = second.A and parse_filename(first.filename)=parse_filename(second.filename)")

end = time.perf_counter()
print(end-start)
start = time.perf_counter()
print(df.fetchdf().shape)
end = time.perf_counter()
print(end-start)

```

### OS:

x86_64

### DuckDB Version:

1.1.1

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Daniel StreitbÃ¼rger

### Affiliation:

ipoque GmbH

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot share the data sets because they are confidential

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have