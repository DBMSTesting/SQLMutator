ID: 14218
Title: Unexpected execution time using OFFSET + LIMIT on larger datasets.
Description:
### What happens?

Unexpected long execution time for paged access using OFFSET -> LIMIT on larger datasets.

**Approach:** 
- Dataset: NY-Taxi yellow data(sub)set with 115,025,861 rows x 19 columns = single ±3.2GB parquet file
- Load data into DuckDB in-memory table using `CREATE TABLE taxi AS SELECT * FROM {file};` in **7162 ms** 
- Get 100 records x 19 columns from different offsets for paged data access using `SELECT * FROM taxi LIMIT 100 OFFSET {offset};`
- All data fits and runs (easily) in memory, 0 bytes swap, ±18GB memory used.

**Results:**
- return 100 rows x 19 cols at offset 0 in 0 ms
- return 100 rows x 19 cols at offset 5.000 in 0 ms
- return 100 rows x 19 cols at offset 5.000.000 in 170 ms
- return 100 rows x 19 cols at offset 50.000.000 in 1547 ms
- return 100 rows x 19cols at offset 115.025.761 in **6414 ms**

Is this an expected behaviour/runtime?

### To Reproduce

```
CREATE TABLE taxi AS SELECT * FROM 'taxi.parquet';

SELECT * FROM taxi LIMIT 100 OFFSET 0;
SELECT * FROM taxi LIMIT 100 OFFSET 5000;
SELECT * FROM taxi LIMIT 100 OFFSET 5000000;
SELECT * FROM taxi LIMIT 100 OFFSET 50000000;
SELECT * FROM taxi LIMIT 100 OFFSET 115025761;
```

### OS:

macOS

### DuckDB Version:

1.1.1

### DuckDB Client:

Python, C

### Hardware:

MBpro M2 32GB

### Full Name:

Thomas Zeutschler

### Affiliation:

BARC Germany (Analysts for Data, Analytics, CPM)

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [ ] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have