ID: 16319
Title: Out of memory error when unnesting rows with a lot of data
Description:
### What happens?

When unnesting rows of data that are very large - like, each unnested object itself is quite large - duckdb doesn't seem to be spilling to disk or whatever other memory mitigation strategies there are.

I've tested with threads=1 and that didn't help.

### To Reproduce

To create the test data:
(this will create a 9.4GB file called "data.jsonl")
```sh
#!/bin/sh

rm -f "data.jsonl"

data=$(yes | tr -d '\n' | head -c 10000000)
for i in $(seq 1 1000); do
  echo "{\"id\": \"$i\", \"data\": [\"$data\"]}" >> "data.jsonl"
done
```

To reproduce:
(the memory limit is just to have a limit to cross in case you have lots of memory on your machine)

```python
SET memory_limit = '10GB';
CREATE OR REPLACE TABLE big_table AS (
  SELECT id
  FROM
    'data.jsonl',
    UNNEST(data) AS u (data)
);
```

Without the unnest bit, this test case works fine. (I know this sample code doesn't actually use the unnested data anyway - my real use case does want to use it, but this was the simplest reproducer I could make.)

### OS:

Linux (Ubuntu 24.04.2 x86_64)

### DuckDB Version:

1.2.0

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Michael Terry

### Affiliation:

Boston Children's Hospital

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have