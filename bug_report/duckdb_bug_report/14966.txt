ID: 14966
Title: DuckDB + Docker huge amount of memory taken
Description:
### What happens?

## The issue 

We want to use Duck DB to process data from one point to another. 
We setup a pipeline using Python. Everything runs smoothly on a VM ( Ubuntu, 16 CPUs, 64Go RAM) or our local Machines (Mac arm with 8 CPUs and 16Go RAM).

As the pipeline ran smoothly, we wanted to industrialize it and put it into a Docker container using a robust POD (32 CPUs, 512Go RAM). But, when we try to make it run on this container, we encounter OOMKilled error for the same data.

## What we want to do

We want to extract, transform, and load some data from point A (delta files) to point B (a SQL database). To do this, we are using several threads that will handle our data. Each thread is responsible for handling some batch of the source data (`1_000_000` per batch), we are using 4 python threads locally or in a VM.

Each batch is then processed using the following query. It is responsible for: 
1. Fetching the wanted batch to the original data (delta file, on the network)
2. Makes cross references with the destination data (getting some id)
3. Copy the result into a parquet file we will later insert into our destination db. This is where it we have an issue.

## Technology used

- DuckDB for Python v1.1.3
- Docker with python:3.12-slim as base image
- Extensions used: azure, postgres, delta

## What we tried

To solve the issue, we tried several things:

- Reduce the number of memory available by duckdb, using `SET memory_limit ` => Duckdb is filling up the disk, and the disk latency explodes (despite having solid storage), 512Go are taken by temp files.
- Reducing the number of threads of our pipeline and Duckdb using 'SET thread TO X' => OOMKilled
- Free memory regularly in each thread using `gc` or `malloc_trim(0)` => OOMKilled
- During the build of the image, override [MALLOC_CONF](https://github.com/duckdb/duckdb/issues/9880) => OOMKilled
- Having dedicated storage for the result of the query and tmp storage of Duckdb => Duckdb is filling up the disk, and the disk latency explodes (despite having solid storage)

I will now try to split the query into small ones, storing intermediate results.

### To Reproduce

This the query we want to run (obviously modified).
```SQL
COPY (
  WITH delta_scan AS (
    SELECT 
      * 
    FROM 
      delta_scan('{delta_table}') {where_clause} 
      AND x = '{x}'
      AND (
        CASE WHEN '{Y}' = 'Y' THEN Y IS NULL ELSE Y = '{Y}' END
      ) offset {offset} 
    limit 
      {batch_size}
  ) 
  SELECT 
    --several_ids
    ds.value 
  FROM 
    sql_database s
    JOIN delta_scan AS ds ON (
      --several joins
    ) 
    --more joins
) TO '{tmp_directory}/{table_name}.parquet' (format 'parquet')

```

### OS:

x86_64, arm_

### DuckDB Version:

1.1.3

### DuckDB Client:

Python

### Hardware:

Docker container on azure 32 CPUs, 512 Go RAM

### Full Name:

Adrien Bruyat

### Affiliation:

InTheMemory

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot share the data sets because they are confidential

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have