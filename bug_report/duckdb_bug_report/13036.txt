ID: 13036
Title: Overhead in con.cursor() is enormous
Description:
### What happens?

This issue is closely related to #10223 but warrants a separate issue.

I would consider it a good practice to share a connection, but to create a cursor before a query. If the software is executed in sequence and all results are directly fetched, it should not really matter. But if you are processing the output of multiple queries _and_ don't want to store the result set into memory.

I have noticed that the software running in the 'proper' way by creating a cursor takes ten times longer than stating `cur = con`.

### To Reproduce

The following code **cannot** reproduce the 10x performance hit, it is only 2% here. I have attempted the same on disk, which results in the same kind of statistics.

```
import duckdb
import time

def run_query1(con):
  cur = con.cursor()
  cur.execute(f"SELECT object FROM test;")
  cur.fetchall()

def run_query2(con):
  cur = con
  cur.execute(f"SELECT object FROM test;")
  cur.fetchall()

with duckdb.connect(":memory:") as con:
  con.execute("CREATE TABLE test (object TEXT);")
  for i in range(0, 10000):
    con.execute("INSERT INTO test VALUES ('Hello world');")

  now = time.time()
  for i in range(0, 100000):
    run_query1(con)

  _now = now
  now = time.time()
  print(now - _now)

with duckdb.connect(":memory:") as con:
  con.execute("CREATE TABLE test (object TEXT);")
  for i in range(0, 10000):
    con.execute("INSERT INTO test VALUES ('Hello world');")

  now = time.time()
  for i in range(0, 100000):
    run_query2(con)

  _now = now
  now = time.time()
  print(now - _now)
```

In the code that I am using myself, the only change to get a ten fold performance improvement is marked below. 1000 objects fetched using the load_generator code. Then for several times load_local is ran within the iteration over the 1000 objects.

The performance is 151 seconds using con.cursor() in load_local and 12 seconds without.
```
def load_local(con, clazz: T, limit=None, filter=None) -> List[T]:
    type = getattr(clazz.Meta, 'name', clazz.__name__)

    cur = con.cursor()
    # cur = con
    try:
        if filter is not None:
            cur.execute(f"SELECT object FROM {type} WHERE id = ?;", (filter,))
        elif limit is not None:
            cur.execute(f"SELECT object FROM {type} LIMIT {limit};")
        else:
            cur.execute(f"SELECT object FROM {type};")
    except:
        return []

    objs: List[T] = []
    for xml, in cur.fetchall():
        if isinstance(xml, str):
            obj = parser.from_string(xml, clazz)
        else:
            obj = parser.from_bytes(xml, clazz)
        objs.append(obj)

    return objs

def load_generator(con, clazz, limit=None, filter=None):
    type = getattr(clazz.Meta, 'name', clazz.__name__)

    cur = con.cursor()
    try:
        if filter is not None:
            cur.execute(f"SELECT object FROM {type} WHERE id = ?;", (filter,))
        elif limit is not None:
            cur.execute(f"SELECT object FROM {type} LIMIT {limit};")
        else:
            cur.execute(f"SELECT object FROM {type};")
    except:
        return

    while True:
        xml = cur.fetchone()
        if xml is None:
            break
        if isinstance(xml[0], str):
            yield parser.from_string(xml[0], clazz)
        else:
            yield parser.from_bytes(xml[0], clazz)
```

### OS:

Linux

### DuckDB Version:

duckdb-1.0.1.dev2941

### DuckDB Client:

Python

### Full Name:

Stefan de Konink

### Affiliation:

Stichting OpenGeo

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have