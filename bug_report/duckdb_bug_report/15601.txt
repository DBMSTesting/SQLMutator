ID: 15601
Title: JSON reader fails with duplicate column name when reading multiple JSON files of slightly different casing
Description:
### What happens?

When reading multiple JSON files using a single FROM "fragment*.json" query, DuckDB throws a duplicate column name "Image_1" error. It appears that columns (JSON object keys) with differing case—e.g., "Image" vs. "image"—may trigger this collision.

### To Reproduce


I have two JSON files with partially overlapping schemas. One file uses a key `"Image"`, and the other uses `"image"`, plus there is a key named `"Image_1"` in the second file. When I attempt to query all JSON files at once using a wildcard pattern, DuckDB complains about a duplicate column name.

**File: `fragment1.json`**
```json
{"id":"19276","Consignee":"zz af9","Time":"30-04-2024 10:06:21","DocketNumber":"6364174","Image":["abc"],"Time_1":"30-04-2024 09:32:13"}
{"id":"19281","Consignee":"zz af9","Time":"30-04-2024 11:38:13","DocketNumber":"6364181","Image":["def"],"Time_1":"30-04-2024 11:01:27"}
```

**File: `fragment2.json`**
```json
{"id":"19421","Comments":null,"Consignee":"zz zz","Driver":null,"Time":"30-04-2024 17:04:01","image":["zz"],"DocketNumber":"11","Image_1":["zzz"],"Time_1":"30-04-2024 17:00:59"}
{"id":"19426","Comments":"delivered","Consignee":null,"Driver":"zz zz","Time":"01-05-2024 08:31:27","image":["zz"],"DocketNumber":"zxc","Image_1":["zzz"],"Time_1":"01-05-2024 08:31:01"}
{"id":"8182","Comments":null,"Consignee":"","Driver":null,"Time":"26-03-2024 18:03:33","image":null,"signature":["xx"],"DocketNumber":"x D xc","Image_1":null,"Time_1":"26-03-2024 18:03:33"}
```

### Python script to reproduce

```python
import duckdb
import os

def main():
    # Create two JSON files with the sample data
    with open('fragment1.json', 'w') as f:
        f.write('''\
{"id":"19276","Consignee":"zz af9","Time":"30-04-2024 10:06:21","DocketNumber":"6364174","Image":["abc"],"Time_1":"30-04-2024 09:32:13"}
{"id":"19281","Consignee":"zz af9","Time":"30-04-2024 11:38:13","DocketNumber":"6364181","Image":["def"],"Time_1":"30-04-2024 11:01:27"}
''')

    with open('fragment2.json', 'w') as f:
        f.write('''\
{"id":"19421","Comments":null,"Consignee":"zz zz","Driver":null,"Time":"30-04-2024 17:04:01","image":["zz"],"DocketNumber":"11","Image_1":["zzz"],"Time_1":"30-04-2024 17:00:59"}
{"id":"19426","Comments":"delivered","Consignee":null,"Driver":"zz zz","Time":"01-05-2024 08:31:27","image":["zz"],"DocketNumber":"zxc","Image_1":["zzz"],"Time_1":"01-05-2024 08:31:01"}
{"id":"8182","Comments":null,"Consignee":"","Driver":null,"Time":"26-03-2024 18:03:33","image":null,"signature":["xx"],"DocketNumber":"x D xc","Image_1":null,"Time_1":"26-03-2024 18:03:33"}
''')

    con = duckdb.connect()
    try:
        # This query should fail due to a duplicate column name
        con.execute("SELECT * FROM 'fragment*.json';")
        results = con.fetchall()
        print("Query succeeded (unexpectedly). Results:", results)
    except Exception as e:
        print("Query failed as expected with error:")
        print(e)

    # Cleanup test files
    os.remove('fragment1.json')
    os.remove('fragment2.json')

if __name__ == "__main__":
    main()
```

### Expected vs. Actual Behavior

- **Expected**: DuckDB should read all rows from both JSON files, merging columns appropriately (with `NULL` where columns are missing).
- **Actual**: A `duplicate column name "Image_1"` error appears:
  ```Binder Error: table "fragment*.json" has duplicate column name "Image_1"```

### OS:

Windows 11 amd64

### DuckDB Version:

1.1.3

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Maitland Marshall

### Affiliation:

MAIT DEV

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have