ID: 14910
Title: to_timestamp function is 5x slower than make_timestamp
Description:
### What happens?

I'm aggregating 4B records with duckdb.
```
SELECT
          channel_id,
          cast(to_timestamp(end_time) as date) AS day,
          sum(end_time - start_time) AS total_time
      FROM read_parquet('large_dataset.parquet/*.parquet')
      GROUP BY channel_id, day

Run Time (s): real 115.647 user 954.673628 sys 32.334959
```

To compare:
```
SELECT
          channel_id,
          cast(make_timestamp(end_time * 1000000) as date) AS day,
          sum(end_time - start_time) AS total_time
      FROM read_parquet('large_dataset.parquet/*.parquet')
      GROUP BY channel_id, day

Run Time (s): real 28.038 user 160.199987 sys 29.989788
```



### To Reproduce

Sample Spark script to generate test data
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    expr, 
    rand, 
    col,
    floor,
    current_timestamp,
    unix_timestamp,
    lit
)
import time

# Initialize Spark Session with appropriate configurations
spark = SparkSession.builder \
    .appName("Generate 4B Records") \
    .config("spark.sql.shuffle.partitions", 1000) \
    .config("spark.default.parallelism", 1000) \
    .config("spark.driver.memory", "16G") \
    .master("local[*]") \
    .getOrCreate()

# Number of records to generate (4 billion)
num_records = 4_000_000_000

# Calculate timestamp ranges for last 7 days using Python's time module
current_ts = int(time.time())
seven_days_ago_ts = current_ts - (7 * 24 * 3600)  # 7 days in seconds

# Create a base DataFrame with the desired number of rows
df = spark.range(0, num_records, numPartitions=1000)

# Add columns with the specified requirements using built-in functions
df = df.select(
    expr("uuid()").alias("device_id"),
    expr("uuid()").alias("user_id"),
    (rand() * 249 + 1).cast("int").alias("channel_id"),
    floor(lit(seven_days_ago_ts) + (rand() * lit(current_ts - seven_days_ago_ts))).cast("int").alias("end_time"),
    (floor(lit(seven_days_ago_ts) + (rand() * lit(current_ts - seven_days_ago_ts))) - (rand() * 3600).cast("int")).alias("start_time"),
    lit("import").alias("type")
)

# Write the DataFrame to Parquet format
output_path = "large_dataset.parquet"
df.write \
    .mode("overwrite") \
    .parquet(output_path)
```

### OS:

MacOS 15.1

### DuckDB Version:

1.1.3

### DuckDB Client:

CLI / Python

### Hardware:

_No response_

### Full Name:

Maciej Brynski

### Affiliation:

Cledar

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have