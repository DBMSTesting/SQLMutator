ID: 15114
Title: Avoid repartitioning out-of-core hash joins if very, very skewed 
Description:
... and use BufferAllocator to buffer data for Parquet instead of `ColumnDataAllocatorType::HYBRID`.

I ran into this while joining two Parquet files - one of which had only one distinct join key. Repartitioning doesn't help, as this will only cause an OOM exception later, so we shouldn't repartition at all, and fail early (or succeed if it _just_ fits in memory!).

I've also changed the allocator type for buffering data, as I found this caused excessive spilling when we were on the edge of the memory limit. We're now routing allocations through the buffer allocator, but not allowing them to spill.

This sped up the Parquet join I was doing from ~40s to ~6s.