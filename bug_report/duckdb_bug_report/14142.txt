ID: 14142
Title: Out of memory error with temp directory even though there's disk space
Description:
### What happens?

I'm trying to merge a few large CSV files into a single Parquet file. I've set `temp_directory` to a folder on disk (`/tmp/duckdb`). I'm getting this error:

```
  0% ▕                                                            ▏ Out of Memory Error: failed to offload data block of size 34.7 MiB (187.3 GiB/187.3 GiB used).
This limit was set by the 'max_temp_directory_size' setting.
By default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.
You can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'
```

I didn't set `max_temp_directory_size` and my understanding is that it defaults to the amount of free space on the disk. There should be plenty of space:

![image](https://github.com/user-attachments/assets/8de78d52-2d13-477a-a641-2dbf74272993)


### To Reproduce

```
D set temp_directory='/tmp/duckdb';
```

and run a query against a very big dataset.

### OS:

macOS 14.6.1 (23G93)

### DuckDB Version:

v1.1.1 af39bd0dcf

### DuckDB Client:

Shell

### Hardware:

Apple M1 (arm64)

### Full Name:

Robert Martin

### Affiliation:

Robert Martin

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have