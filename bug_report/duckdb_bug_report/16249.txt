ID: 16249
Title: Out of Memory Error When Summarizing Large Parquet File
Description:
### What happens?

Hi Team,

I am encountering an issue while working with a large Parquet file and following the instructions from this [tutorial ](https://medium.com/inthepipeline/from-zero-to-dbt-how-to-analyze-and-build-data-models-from-spotifys-million-playlist-data-241c3d8c9b5d). After converting the JSON files from the ZIP to a single Parquet file, the resultant file size is approximately 5 GB.

Issue:
When I attempt to summarize the data, I receive an "Out of Memory" error, despite having set both the memory_limit and temp_directory configurations as recommended.

Steps to Reproduce:

Follow the steps in the linked blog post to convert the data into a Parquet file.
Attempt to summarize the data in the Parquet file using the same environment.

Any insights or solutions for efficiently handling large data operations would be greatly appreciated. Thank you for your assistance!

Best Regards,

### To Reproduce

```
 SUMMARIZE SELECT * FROM './playlists.parquet' USING SAMPLE 10%;
```

### OS:

Ubuntu 

### DuckDB Version:

1.20

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Razi Marjani

### Affiliation:

Cafebazaar

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [ ] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [ ] Yes, I have