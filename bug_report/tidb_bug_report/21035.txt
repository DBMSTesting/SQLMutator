ID: 21035
Title: QPS drop for a while when benching TPCC due to different plans
Description:
## Bug Report

### 1. Minimal reproduce step (Required)

1. deploy cluster using v4.0.8
```
pd_servers:
  - host: 172.16.5.34
    client_port: 20002
    peer_port: 20003

tidb_servers:
  - host: 172.16.5.34
    port: 20000
    status_port: 20001

tikv_servers:
  - host: 172.16.5.35
    port: 20004
    status_port: 20005
    deploy_dir: "/nvme1n1/cluster-lxt/deploy"
    data_dir: "/nvme1n1/cluster-lxt/data"
    resource_control:
      memory_limit: "32G"
```
2. replace tikv binary. refer to https://github.com/tikv/tikv/pull/8908
2. enable zstd for bottommost level sst and enable dict compression. 
```
[rocksdb.defaultcf]
block-size = "4KB"
zstd-dict-size = 4096
zstd-sample-size = 8388608
[rocksdb.writecf]
block-size = "4KB"
zstd-dict-size = 4096
zstd-sample-size = 8388608
```
3. prepare and run tpcc
```
go-tpc tpcc -P 20000 --warehouses 1000 --parts 1 prepare
sleep 1000
go-tpc tpcc -P 20000 -T 200 --warehouses 1000 --parts 1 run
```

### 2. What did you expect to see? (Required)

Stable qps

### 3. What did you see instead (Required)

Sometimes qps drops by 20%~30% and we seen that plan has been changed from index_join to hash_join (refer to dashboard).

![image](https://user-images.githubusercontent.com/18193301/99033536-15814100-25b6-11eb-9e1b-590fd60d249d.png)


### 4. What is your TiDB version? (Required)

v4.0.8

