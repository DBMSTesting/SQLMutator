ID: 32287
Title: TiDB Server crashing w/ OOM despite the tidb_mem_quota_query limit
Description:
## Bug Report


### 1. Minimal reproduce step (Required)

huge hash join 


### 2. What did you expect to see? (Required)

We are mimicking our production workloads to a dev TiDB cluster. One of the biggest queries is a huge hash join, which can consume ~50GB of memory. We reduce the `tidb_mem_quota_query` to 1GB to avoid system's oom-killing, but the setting didn't work.

After diving deeper into the `hashjoinExecutor` implementation, we figure out that's because below memory usages are not counted in the memory tracker:

1. The hash table used for joining: https://github.com/pingcap/tidb/blob/master/executor/hash_table.go#L187
2. The entryStore for above hash table: https://github.com/pingcap/tidb/blob/master/executor/hash_table.go#L324
3. The newly created chunks: https://github.com/pingcap/tidb/blob/master/executor/join.go#L281
Besides above `func (worker *copIteratorWorker) handleCopResponse` can take another 2~4 GB in our case.

I am attaching a heap profiling to this ticket, please check it out. We took the profiling when the TiDB server was still alive, taking around 15.8GB heap memory, but eventually, it can consume ~50GB.

### 3. What did you see instead (Required)

tidb server get killed by system's oom-killer

### 4. What is your TiDB version? (Required)

5.2.2 on kubernetes

<!-- Paste the output of SELECT tidb_version() -->

