ID: 60747
Title: INSERT INTO s3 function fails to perform multipart upload when setting custom metadata headers 
Description:
**Describe what's wrong**

When executing an `INSERT INTO s3(...)` query, if a metadata header (`x-amz-meta-`) is set and the resulting file is large enough to produce a multipart upload (50 MB by default), the insertion & upload to S3 fails with the following error:

```
[Error] Unable to parse ExceptionName: InvalidArgument Message: Metadata cannot be specified in this context. (S3_ERROR)
```

**Does it reproduce on the most recent release?**

Yes. Tested with both versions `23.12` and master, fails in both.

**How to reproduce**

All default settings, try to upload the file to actual S3, doesn't happen in MINIO

```
INSERT INTO FUNCTION s3('https://my-bucket.s3.eu-west-3.amazonaws.com/myfile.csv', 'REDACTED', 'REDACTED', 'CSV', headers('x-amz-meta-my-meta-header' = 'value')) SELECT *
FROM generateRandom('uuid UUID, context Text, dt Datetime')
LIMIT 1000000
```

This will generate a 50+ MB CSV file that will trigger a multipart upload, and then will fail with the error above.

**Expected behavior**

The file is upload successfully to S3 if I specify custom metadata headers even if a multipart upload is used.

**Error message and/or stacktrace**

```
Code: 499. DB::Exception: Unable to parse ExceptionName: InvalidArgument Message: Metadata cannot be specified in this context. (S3_ERROR) (version 23.12.2.59 (official build))
stack_trace:
0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000bae58c8 in /usr/bin/clickhouse
1. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::WriteBufferFromS3::writePart(DB::WriteBufferFromS3::PartData&&)::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000000f0a90c4 in /usr/bin/clickhouse
2. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::WriteBufferFromS3::TaskTracker::add(std::function<void ()>&&)::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x000000000f0acc84 in /usr/bin/clickhouse
3. std::__packaged_task_func<std::function<std::future<void> (std::function<void ()>&&, Priority)> DB::threadPoolCallbackRunner<void, std::function<void ()>>(ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>&, String const&)::'lambda'(std::function<void ()>&&, Priority)::operator()(std::function<void ()>&&, Priority)::'lambda'(), std::allocator<std::function<std::future<void> (std::function<void ()>&&, Priority)> DB::threadPoolCallbackRunner<void, std::function<void ()>>(ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>&, String const&)::'lambda'(std::function<void ()>&&, Priority)::operator()(std::function<void ()>&&, Priority)::'lambda'()>, void ()>::operator()() @ 0x000000000f089fd0 in /usr/bin/clickhouse
4. std::packaged_task<void ()>::operator()() @ 0x000000000e983260 in /usr/bin/clickhouse
5. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000bbb0c10 in /usr/bin/clickhouse
6. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x000000000bbb3d00 in /usr/bin/clickhouse
7. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000bbb2d08 in /usr/bin/clickhouse
8. start_thread @ 0x0000000000007624 in /usr/lib/aarch64-linux-gnu/libpthread-2.31.so
9. ? @ 0x00000000000d162c in /usr/lib/aarch64-linux-gnu/libc-2.31.so
```

**Additional context**

Only happens when uploading data to actual S3, haven't been able to replicate it with MinIO or GCS
