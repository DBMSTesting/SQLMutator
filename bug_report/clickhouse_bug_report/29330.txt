ID: 29330
Title: Unable to copy millions of rows in clickhouse (table X1 to table X2)
Description:
I am seeing one strange behavior when copying rows from clickhouse tableX1 to tableX2. Here  I am copying rows in chunk of 5 million from tableX1 to tableX2. Copying of rows works initially but later inserted numbers are not same as expected !!! For Example-After two cycle of insertion, I am expecting 10 millions rows but total inserted rows was 9995850 (less than 10 million). I saw that clickhouse is always inserting less number of rows and total inserted row count differs on every retry.


`232655c9808c :) select count(*) from tableX2;

SELECT count(*)
FROM tableX2

Query id: 31f12170-7302-4130-88c2-a8c8e086e376

┌─count()─┐
│       0 │
└─────────┘

1 rows in set. Elapsed: 0.012 sec. 

232655c9808c :) Insert into tableX2 (date, time,tags_id, value, context) select * from tableX1 limit 0, 5000000;

INSERT INTO tableX2 (date, time, tags_id, value, context) SELECT *
FROM tableX1
LIMIT 0, 5000000

Query id: 34a004aa-0175-4b65-9631-14e1145753c2

Ok.

0 rows in set. Elapsed: 2.904 sec. Processed 4.88 million rows, 484.92 MB (1.68 million rows/s., 167.01 MB/s.) 

232655c9808c :) select count(*) from tableX2;

SELECT count(*)
FROM tableX2

Query id: a3a92615-9fd0-43c1-bb24-e8fdcd8db0e6

┌─count()─┐
│ 5000000 │
└─────────┘

1 rows in set. Elapsed: 0.016 sec. 

232655c9808c :) Insert into tableX2 (date, time,tags_id, value, context) select * from tableX1 limit 5000000, 5000000;

INSERT INTO tableX2 (date, time, tags_id, value, context) SELECT *
FROM tableX1
LIMIT 5000000, 5000000

Query id: 28e4dd37-376f-4c9a-bb6f-eb4f9daf5d27

Ok.

0 rows in set. Elapsed: 3.555 sec. Processed 9.80 million rows, 973.08 MB (2.76 million rows/s., 273.75 MB/s.) 

232655c9808c :) select count(*) from tableX2;

SELECT count(*)
FROM tableX2

Query id: 7d055af6-4f0a-4932-bb6d-4bb59f3c9eea

┌─count()─┐
│ **9995850** │
└─────────┘

1 rows in set. Elapsed: 0.005 sec. 

232655c9808c :)`


**How to reproduce**

*Which Clickhouse server version to use*
20.12.4.5 version

*Which interface to use, if matters*
clickhouse-client

*Any further  information that could be relevant*
1. TableX1 has monthly partition and tableX2 has daily partition. We are saving data only for one month
2. I tried to order the data before copying but still resulted in less number of rows
3. I do not see any out of memory issue while inserting 5 millions rows
4. Copy works perfectly fine when all rows are copied. Example- Insert into tableX2 (date, time,tags_id, value, context) select * from tableX1. But not when rows are copied in batches of millions(5-Million)
4. I am looking to scale this copy logic on a production server that may have ~50 billion of rows.




