ID: 8152
Title: Sizes of columns doesn't match error under 19.17.5.18
Description:
Upgrading from 19.16.3.6 to 19.17.5.18, Kafka consumption and inserts fail with the following exceptions:

```
2019.12.11 12:54:16.442070 [ 25 ] {} <Error> virtual DB::UnionBlockInputStream::~UnionBlockInputStream(): Code: 9, e.displayText() = DB::Exception: Sizes of columns doesn't match: Timestamp: 65546, _timestamp: 0, Stack trace:

0. 0x555e83779690 StackTrace::StackTrace() /usr/bin/clickhouse
1. 0x555e83779465 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse
2. 0x555e83469a0e ? /usr/bin/clickhouse
3. 0x555e873e145e DB::KafkaBlockInputStream::readImpl() /usr/bin/clickhouse
4. 0x555e869d25ca DB::IBlockInputStream::read() /usr/bin/clickhouse
5. 0x555e86b51ec2 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::loop(unsigned long) /usr/bin/clickhouse
6. 0x555e86b52585 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /usr/bin/clickhouse
7. 0x555e86b52efd ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /usr/bin/clickhouse
8. 0x555e837c3c4c ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse
9. 0x555e8954dec0 ? /usr/bin/clickhouse
10. 0x7f182a5eedd5 start_thread /usr/lib64/libpthread-2.17.so
11. 0x7f1829f0bead __clone /usr/lib64/libc-2.17.so
 (version 19.17.5.18 (official build))
2019.12.11 12:54:16.494772 [ 25 ] {} <Error> void DB::StorageKafka::threadFunc(): Code: 9, e.displayText() = DB::Exception: Sizes of columns doesn't match: Timestamp: 65547, _timestamp: 0, Stack trace:

0. 0x555e83779690 StackTrace::StackTrace() /usr/bin/clickhouse
1. 0x555e83779465 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse
2. 0x555e83469a0e ? /usr/bin/clickhouse
3. 0x555e873e145e DB::KafkaBlockInputStream::readImpl() /usr/bin/clickhouse
4. 0x555e869d25ca DB::IBlockInputStream::read() /usr/bin/clickhouse
5. 0x555e86b51ec2 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::loop(unsigned long) /usr/bin/clickhouse
6. 0x555e86b52585 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /usr/bin/clickhouse
7. 0x555e86b52efd ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /usr/bin/clickhouse
8. 0x555e837c3c4c ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse
9. 0x555e8954dec0 ? /usr/bin/clickhouse
10. 0x7f182a5eedd5 start_thread /usr/lib64/libpthread-2.17.so
11. 0x7f1829f0bead __clone /usr/lib64/libc-2.17.so
 (version 19.17.5.18 (official build))
```
The upgrade is just a yum upgrade. My table schema is as follows:
```
metrics_qa :) show create table default.histograms_kafka_processor;

SHOW CREATE TABLE default.histograms_kafka_processor

┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE MATERIALIZED VIEW default.histograms_kafka_processor TO default.histograms_dist (`Timestamp` DateTime, `Client` LowCardinality(String), `Path` LowCardinality(String), `Tags.Key` Array(LowCardinality(String)), `Tags.Value` Array(String), `Range` Array(Float32), `Steps` Float32, `IndexNotation` Int32, `Histogram` Array(Float32)) AS SELECT * FROM default.histograms_kafka │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

1 rows in set. Elapsed: 0.003 sec. 

metrics_qa :) show create table default.histograms_kafka;

SHOW CREATE TABLE default.histograms_kafka

┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.histograms_kafka (`Timestamp` DateTime CODEC(DoubleDelta, LZ4), `Client` LowCardinality(String), `Path` LowCardinality(String), `Tags.Key` Array(LowCardinality(String)), `Tags.Value` Array(String) CODEC(LZ4HC(0)), `Range` Array(Float32), `Steps` Float32, `IndexNotation` Int32, `Histogram` Array(Float32)) ENGINE = Kafka SETTINGS kafka_broker_list = 'tst-playkafka1.indeed.net:9092,tst-playlafla2.indeed.net:9092,tst-playkafka3.indeed.net:9092', kafka_topic_list = 'default_prod_qa', kafka_format = 'JSONEachRow', kafka_group_name = 'metrics_qa2', kafka_row_delimiter = '\n', kafka_num_consumers = 2, kafka_skip_broken_messages = 1 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

1 rows in set. Elapsed: 0.002 sec. 

metrics_qa :) show create table default.histograms_dist;

SHOW CREATE TABLE default.histograms_dist

┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.histograms_dist (`Timestamp` DateTime CODEC(DoubleDelta), `Client` LowCardinality(String), `Path` LowCardinality(String), `Tags.Key` Array(LowCardinality(String)), `Tags.Value` Array(String) CODEC(LZ4HC(0)), `Range` Array(Float32), `Steps` Float32, `IndexNotation` Int32, `Histogram` Array(Float32)) ENGINE = Distributed(metrics_qa, default, histograms, rand()) │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

1 rows in set. Elapsed: 0.002 sec. 

metrics_qa :) 
```