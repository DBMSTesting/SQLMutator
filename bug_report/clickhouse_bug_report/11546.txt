ID: 11546
Title: `Memory limit (total) exceeded` during insert leads to partially write
Description:
(you don't have to strictly follow this form)

**Describe the bug**
I use java clickhouse connector to insert data into Clickhouse. Once I inserted 4 rows and got the answer from the clickhouse:
```
Caused by: ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 241, host: clickhouse.prod.env, port: 38123; Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 123.87 GiB (attempt to allocate chunk of 4217748 bytes), maximum: 123.87 GiB (version 19.16.10.44 (official build))
```

(The memory seems to be consumed by another queries).

After that I found that **3 of 4 records** was inserted and *the one* (last in ORDER BY sort) **was not inserted**.

It seems that clickhouse violates block insert atomicity in case of out of memory errors?

IMHO, there should be atomicity in block write - either all rows are inserted or none of rows.