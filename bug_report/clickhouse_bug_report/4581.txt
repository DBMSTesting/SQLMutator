ID: 4581
Title: 19.1.10 invalidate_query runs every 5 seconds if the value that it returns does not change
Description:
I have dictionaries on MergeTree tables, and their invalidate queries check the modification time in the system.parts table.
The dictionary gets created successfully, then after 'lifetime' of seconds the invalidate query runs and gets a value. After another 'lifetime' seconds the invalidate query runs again, and if it gets the same value then the invalidate query starts running every 5 seconds.
I first noticed this on 19.1.7.

The dictionary was created at 17:25:30
```
2019.03.04 17:25:30.114196 [ 34 ] {} <Debug> db.table (SelectExecutor): Key condition: unknown
2019.03.04 17:25:30.114227 [ 34 ] {} <Debug> db.table (SelectExecutor): MinMax index condition: unknown
2019.03.04 17:25:30.114240 [ 34 ] {} <Debug> db.table (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1 marks to read from 1 ranges
2019.03.04 17:25:30.114309 [ 34 ] {} <Trace> db.table (SelectExecutor): Reading approx. 8192 rows with 1 streams
2019.03.04 17:25:30.114341 [ 34 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
```
The first invalidate_query (lifetime=60)
```
2019.03.04 17:26:30.144903 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
2019.03.04 17:26:30.145045 [ 35 ] {} <Trace> Aggregator: Aggregating
2019.03.04 17:26:30.145099 [ 35 ] {} <Trace> Aggregator: Aggregation method: without_key
2019.03.04 17:26:30.145158 [ 35 ] {} <Trace> Aggregator: Aggregated. 1 to 1 rows (from 0.000 MiB) in 0.000 sec. (13472.187 rows/sec., 0.051 MiB/sec.)
2019.03.04 17:26:30.145169 [ 35 ] {} <Trace> Aggregator: Merging aggregated data
2019.03.04 17:26:30.145201 [ 35 ] {} <Trace> virtual DB::MergingAndConvertingBlockInputStream::~MergingAndConvertingBlockInputStream(): Waiting for threads to finish
2019.03.04 17:26:30.145533 [ 35 ] {} <Debug> db.table (SelectExecutor): Key condition: unknown
2019.03.04 17:26:30.145548 [ 35 ] {} <Debug> db.table (SelectExecutor): MinMax index condition: unknown
2019.03.04 17:26:30.145559 [ 35 ] {} <Debug> db.table (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1 marks to read from 1 ranges
2019.03.04 17:26:30.145619 [ 35 ] {} <Trace> db.table (SelectExecutor): Reading approx. 8192 rows with 1 streams
2019.03.04 17:26:30.145642 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
```
The second invalidate_query
```
2019.03.04 17:27:30.153386 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
2019.03.04 17:27:30.153514 [ 35 ] {} <Trace> Aggregator: Aggregating
2019.03.04 17:27:30.153567 [ 35 ] {} <Trace> Aggregator: Aggregation method: without_key
2019.03.04 17:27:30.153612 [ 35 ] {} <Trace> Aggregator: Aggregated. 1 to 1 rows (from 0.000 MiB) in 0.000 sec. (16621.788 rows/sec., 0.063 MiB/sec.)
2019.03.04 17:27:30.153622 [ 35 ] {} <Trace> Aggregator: Merging aggregated data
2019.03.04 17:27:30.153649 [ 35 ] {} <Trace> virtual DB::MergingAndConvertingBlockInputStream::~MergingAndConvertingBlockInputStream(): Waiting for threads to finish
```
Now it runs every 5 seconds
```
2019.03.04 17:27:35.155565 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
2019.03.04 17:27:35.155687 [ 35 ] {} <Trace> Aggregator: Aggregating
2019.03.04 17:27:35.155736 [ 35 ] {} <Trace> Aggregator: Aggregation method: without_key
2019.03.04 17:27:35.155778 [ 35 ] {} <Trace> Aggregator: Aggregated. 1 to 1 rows (from 0.000 MiB) in 0.000 sec. (17969.775 rows/sec., 0.069 MiB/sec.)
2019.03.04 17:27:35.155787 [ 35 ] {} <Trace> Aggregator: Merging aggregated data
2019.03.04 17:27:35.155813 [ 35 ] {} <Trace> virtual DB::MergingAndConvertingBlockInputStream::~MergingAndConvertingBlockInputStream(): Waiting for threads to finish
2019.03.04 17:27:40.157791 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
2019.03.04 17:27:40.157918 [ 35 ] {} <Trace> Aggregator: Aggregating
2019.03.04 17:27:40.157968 [ 35 ] {} <Trace> Aggregator: Aggregation method: without_key
2019.03.04 17:27:40.158012 [ 35 ] {} <Trace> Aggregator: Aggregated. 1 to 1 rows (from 0.000 MiB) in 0.000 sec. (17465.418 rows/sec., 0.067 MiB/sec.)
2019.03.04 17:27:40.158021 [ 35 ] {} <Trace> Aggregator: Merging aggregated data
2019.03.04 17:27:40.158049 [ 35 ] {} <Trace> virtual DB::MergingAndConvertingBlockInputStream::~MergingAndConvertingBlockInputStream(): Waiting for threads to finish
2019.03.04 17:27:45.159989 [ 35 ] {} <Trace> InterpreterSelectQuery: FetchColumns -> Complete
2019.03.04 17:27:45.160107 [ 35 ] {} <Trace> Aggregator: Aggregating
2019.03.04 17:27:45.160157 [ 35 ] {} <Trace> Aggregator: Aggregation method: without_key
2019.03.04 17:27:45.160201 [ 35 ] {} <Trace> Aggregator: Aggregated. 1 to 1 rows (from 0.000 MiB) in 0.000 sec. (17625.804 rows/sec., 0.067 MiB/sec.)
2019.03.04 17:27:45.160210 [ 35 ] {} <Trace> Aggregator: Merging aggregated data
2019.03.04 17:27:45.160237 [ 35 ] {} <Trace> virtual DB::MergingAndConvertingBlockInputStream::~MergingAndConvertingBlockInputStream(): Waiting for threads to finish
```
The definition of the dictionary:
```
    <dictionary>
        <name>table</name>
        <source>
            <clickhouse>
                <host>localhost</host>
                <port>9000</port>
                <user>default</user>
                <password/>
                <db>db</db>
                <table>table</table>
                <invalidate_query>select max(modification_time) from system.parts where database='db' and table='table'</invalidate_query>
            </clickhouse>
        </source>
        <lifetime>60</lifetime>
        <layout><flat/></layout>
        <structure>
            <id>
                <name>key</name>
            </id>
            <attribute>
                <name>name</name>
                <type>String</type>
                <null_value/>
            </attribute>
        </structure>
    </dictionary>
```
The definition of the source table:
```
CREATE TABLE db.table 
(
  key    Int64,
  name   String
)
ENGINE = ReplacingMergeTree 
PARTITION BY tuple ()
ORDER BY key
```