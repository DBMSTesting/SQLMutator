ID: 40292
Title: fix heap buffer overflow by limiting http chunk size
Description:
02403_big_http_chunk_size -- test which make clickhouse crash.
Client sends data with chunked mode. It is up to client what to write to the chunks headers. Client is able to write there 2^64 us chunks size. Clickhouse parse chunks size from stream safely.  Chunks size is passed to the BufferWithOwnMemory::Memory::resize method. Integer overflow happens when Memory calculates how many bytes need to allocate, when padding is added. Turns out that Memory decided to allocate a few bytes when size was huge. After that HTTPChunkedReadBuffer writes a data after allocated memory.

I suggest to bound maximum chunk size which clickhouse accept. So there is new option http_max_chunk_size in the Setting with default value ~~32KB~~ 100G. Whatever implementation of Memory is used, Clickhouse should check the user input and rejects suspicious requests.
Also the implementation of class Memory have to be correct when big numbers are passed. If it is impossible, to add padding, then it throws exception instead wrong result.

In case when client sends chunk size bigger that value server just closes the connection. Unfortunately a client sees some generic exception without proper error message and code in that and similar cases. But server writes log with that event correctly with error log level.

### Changelog category (leave one):
- Not for changelog (changelog entry is not required)


### Changelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):
This bug was found and send through ClickHouse bug-bounty [program](https://github.com/ClickHouse/ClickHouse/issues/38986) by *kiojj*.


> Information about CI checks: https://clickhouse.com/docs/en/development/continuous-integration/
