ID: 44271
Title: WriteBufferFromS3, broken writes for big files
Description:
According to the logs which I collected CH has a problem with WriteBufferFromS3. CH write big files to S3 with multipart upload request. As I see the first part are written with size 33M. 2:500 parts with 17M. The rest parts are only 1M. As a result we have a problems with files bigger 8,5G (33+17*500+1). Only last part could be lesser than 5M, we have 9500 small parts.

This is the description on of the incomplete multipart upload.
[example-3.json.txt](https://github.com/ClickHouse/ClickHouse/files/10238963/example-3.json.txt)


https://github.com/ClickHouse/ClickHouse/pull/42833
https://github.com/ClickHouse/ClickHouse/commit/32194c1200ce48a2a90528a282791c985c794549
This commit appears in 22.11. I thinks there is a bug with max_upload_part_size which happens to be 0, that explains the parts sizes. This issue has been fixed here https://github.com/ClickHouse/ClickHouse/pull/44335


Problems:
- buffer could produce parts >5G, would be generated 5G + 1m parts. Will be covered by https://github.com/ClickHouse/ClickHouse/pull/44869
- api extension preFinalize is not safe for use, after it is called the buffer must be closed, any writes after is lost.
- there is no retries for part's upload
- there is no cancelation for upload