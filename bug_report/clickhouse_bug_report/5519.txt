ID: 5519
Title: Error inserting csv data
Description:
**Describe the bug**
I encounter a segfault while inserting data from csv files.

**How to reproduce**
* ClickHouse 19.5.3.8 and above
* max_partition_per_insert_block: 10000
* I'm using infi.clickhouse_orm to build my table:
```
class CredentialPlus(Model):
    """
    Class for modelling `credential` and `leak` at the same time.
    """
    user = StringField()
    domain = StringField()
    password = StringField()
    has_been_cleaned = UInt8Field()
    # leak part
    leak_name = StringField()
    leak_desc = StringField()
    breach_date = DateField()
    publish_date = DateField()

    engine = MergeTree(
        partition_key=('domain',),
        order_by=['domain', 'breach_date', 'user'],
        index_granularity=8192
    )
```
* example: `user, domain, password, 0, name, desc, date1, date2`
* `db.raw("INSERT INTO credentialplus format csv %s" % (csv_content))`

**Expected behavior**
Inserting the data into the database.

**Error message and/or stacktrace**
```
 (version 19.5.3.8 (official build))
2019.06.03 11:53:04.256403 [ 33 ] {} <Error> BaseDaemon: ########################################
2019.06.03 11:53:04.256453 [ 33 ] {} <Error> BaseDaemon: (version 19.5.3.8 (official build)) (from thread 31) Received signal Segmentation fault (11).
2019.06.03 11:53:04.256471 [ 33 ] {} <Error> BaseDaemon: Address: NULL pointer.
2019.06.03 11:53:04.256479 [ 33 ] {} <Error> BaseDaemon: Access: read.
2019.06.03 11:53:04.256487 [ 33 ] {} <Error> BaseDaemon: Unknown si_code.
2019.06.03 11:53:04.326511 [ 33 ] {} <Error> BaseDaemon: 0. /usr/bin/clickhouse-server(DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)+0x1c) [0x6a196bc]
2019.06.03 11:53:04.326603 [ 33 ] {} <Error> BaseDaemon: 1. /usr/bin/clickhouse-server(DB::MergeTreeDataPart::MinMaxIndex::update(DB::Block const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)+0x10e) [0x6b7708e]
2019.06.03 11:53:04.326626 [ 33 ] {} <Error> BaseDaemon: 2. /usr/bin/clickhouse-server(DB::MergeTreeDataWriter::writeTempPart(DB::BlockWithPartition&)+0xbe) [0x6b9b11e]
2019.06.03 11:53:04.326636 [ 33 ] {} <Error> BaseDaemon: 3. /usr/bin/clickhouse-server(DB::MergeTreeBlockOutputStream::write(DB::Block const&)+0xf1) [0x6b47c71]
2019.06.03 11:53:04.326647 [ 33 ] {} <Error> BaseDaemon: 4. /usr/bin/clickhouse-server(DB::PushingToViewsBlockOutputStream::write(DB::Block const&)+0x43) [0x6d4ffb3]
2019.06.03 11:53:04.326657 [ 33 ] {} <Error> BaseDaemon: 5. /usr/bin/clickhouse-server(DB::SquashingBlockOutputStream::finalize()+0x13e) [0x6d591be]
2019.06.03 11:53:04.326667 [ 33 ] {} <Error> BaseDaemon: 6. /usr/bin/clickhouse-server(DB::SquashingBlockOutputStream::writeSuffix()+0x11) [0x6d593e1]
2019.06.03 11:53:04.326680 [ 33 ] {} <Error> BaseDaemon: 7. /usr/bin/clickhouse-server(DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::atomic<bool>*)+0x6af) [0x679812f]
2019.06.03 11:53:04.326699 [ 33 ] {} <Error> BaseDaemon: 8. /usr/bin/clickhouse-server(DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>)+0x200) [0x69c6cc0]
2019.06.03 11:53:04.326716 [ 33 ] {} <Error> BaseDaemon: 9. /usr/bin/clickhouse-server(DB::HTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&)+0x3636) [0x3799576]
2019.06.03 11:53:04.326727 [ 33 ] {} <Error> BaseDaemon: 10. /usr/bin/clickhouse-server(DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&)+0x3f1) [0x379b541]
2019.06.03 11:53:04.326744 [ 33 ] {} <Error> BaseDaemon: 11. /usr/bin/clickhouse-server(Poco::Net::HTTPServerConnection::run()+0x32c) [0x7506d6c]
2019.06.03 11:53:04.326753 [ 33 ] {} <Error> BaseDaemon: 12. /usr/bin/clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x7500d6f]
2019.06.03 11:53:04.326763 [ 33 ] {} <Error> BaseDaemon: 13. /usr/bin/clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0xe9) [0x75014a9]
2019.06.03 11:53:04.326773 [ 33 ] {} <Error> BaseDaemon: 14. /usr/bin/clickhouse-server(Poco::PooledThread::run()+0x81) [0x75b45e1]
2019.06.03 11:53:04.326783 [ 33 ] {} <Error> BaseDaemon: 15. /usr/bin/clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x75b07a8]
2019.06.03 11:53:04.326792 [ 33 ] {} <Error> BaseDaemon: 16. /usr/bin/clickhouse-server() [0xb27522f]
2019.06.03 11:53:04.326800 [ 33 ] {} <Error> BaseDaemon: 17. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db) [0x7fee902ed6db]
```

**Additional context**
I can insert a 9MB csv file and below. My next one to be inserted is a 22MB and it crashes. Same for bigger files. My max size is 64MB, and I split bigger files to parts of 64MB. My biggest files are >15GB.
