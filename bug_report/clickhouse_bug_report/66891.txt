ID: 66891
Title: Segmentation Fault in ClickHouse 24.6.2.17 with Concurrent Queries on AWS ECS Using Official ARM64 Docker Image
Description:
Just setting up a playground with a single instance on AWS.

**How to reproduce**

* 24.6.2.17
* ECS task in docker container using the official clickhouse-server arm64 image
* no settings changed (logger trace + s3 use_environment_variables but I see it is true by default and I didn't have to add it)
* Queries to run that lead to an unexpected result (ran in parallel as a test):

```
SELECT * FROM system.tables FORMAT JSON;
SELECT * FROM s3( 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', 'CSVWithNames' ) LIMIT 5 FORMAT JSON;
```

```
2024.07.22 13:33:12.964054 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Debug> executeQuery: (from 127.0.0.1:35856) SELECT * FROM system.tables FORMAT JSON (stage: Complete)
2024.07.22 13:33:12.964207 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Debug> executeQuery: (from 127.0.0.1:35862) SELECT * FROM s3( 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', 'CSVWithNames' ) LIMIT 5 FORMAT JSON (stage: Complete)
2024.07.22 13:33:12.964539 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Trace> Planner: Query to stage Complete
2024.07.22 13:33:12.964854 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Trace> Planner: Query from stage FetchColumns to stage Complete
2024.07.22 13:33:12.968612 [ 619 ] {70dd095d-4079-463f-861e-b890cdc17fb5::202407_1_771_154} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 5818 rows, containing 946 columns (946 merged, 0 gathered) in 0.246624492 sec., 23590.51995533355 rows/sec., 16.28 MiB/sec.
2024.07.22 13:33:12.972943 [ 619 ] {70dd095d-4079-463f-861e-b890cdc17fb5::202407_1_771_154} <Trace> MergedBlockOutputStream: filled checksums 202407_1_771_154 (state Temporary)
2024.07.22 13:33:12.974440 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Warning> AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider: Token file must be specified to use STS AssumeRole web identity creds provider.
2024.07.22 13:33:12.974503 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Information> AWSClient: SSOBearerTokenProvider: Setting sso bearerToken provider to read config from default
2024.07.22 13:33:12.974517 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Trace> SSOCredentialsProvider: Setting sso credentials provider to read config from default
2024.07.22 13:33:12.974528 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Debug> S3CredentialsProviderChain: The environment variable value AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is /v2/credentials/[omitted]
2024.07.22 13:33:12.974535 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Debug> S3CredentialsProviderChain: The environment variable value AWS_CONTAINER_CREDENTIALS_FULL_URI is
2024.07.22 13:33:12.974541 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Debug> S3CredentialsProviderChain: The environment variable value AWS_EC2_METADATA_DISABLED is
2024.07.22 13:33:12.974553 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Information> AWSClient: GeneralHTTPCredentialsProvider: Creating GeneralHTTPCredentialsProvider with refresh rate 300000
2024.07.22 13:33:12.974576 [ 48 ] {3c0084a5-69b2-4a8f-a1e9-de104ea5275d} <Information> AWSClient: ECSCredentialsClient: Creating AWSHttpResourceClient with max connections 2 and scheme http
2024.07.22 13:33:12.974687 [ 46 ] {} <Trace> BaseDaemon: Received signal 11
2024.07.22 13:33:12.974871 [ 967 ] {} <Fatal> BaseDaemon: ########## Short fault info ############
2024.07.22 13:33:12.974902 [ 967 ] {} <Fatal> BaseDaemon: (version 24.6.2.17 (official build), build id: 8A6C5338344E66EC854D13093504080DADD87D66, git hash: 5710a8b5c0cf20504ee671c64f6fdb6627c696b5) (from thread 48) Received signal 11
2024.07.22 13:33:12.974919 [ 967 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault
2024.07.22 13:33:12.974928 [ 967 ] {} <Fatal> BaseDaemon: Address: 0xaaaac3ac36a8. Access: . Attempted access has violated the permissions assigned to the memory area.
2024.07.22 13:33:12.974939 [ 967 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000aaaaba44c788 0x0000ffff842f3830
2024.07.22 13:33:12.974948 [ 967 ] {} <Fatal> BaseDaemon: ########################################
2024.07.22 13:33:12.974963 [ 967 ] {} <Fatal> BaseDaemon: (version 24.6.2.17 (official build), build id: 8A6C5338344E66EC854D13093504080DADD87D66, git hash: 5710a8b5c0cf20504ee671c64f6fdb6627c696b5) (from thread 48) (query_id: 3c0084a5-69b2-4a8f-a1e9-de104ea5275d) (query: SELECT *
FROM s3(
   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv',
   'CSVWithNames'
)
LIMIT 5
FORMAT JSON) Received signal Segmentation fault (11)
2024.07.22 13:33:12.974982 [ 967 ] {} <Fatal> BaseDaemon: Address: 0xaaaac3ac36a8. Access: . Attempted access has violated the permissions assigned to the memory area.
2024.07.22 13:33:12.974989 [ 967 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000aaaaba44c788 0x0000ffff842f3830
2024.07.22 13:33:12.975047 [ 967 ] {} <Fatal> BaseDaemon: 0. signalHandler(int, siginfo_t*, void*) @ 0x000000000c5fc788
2024.07.22 13:33:12.975057 [ 967 ] {} <Fatal> BaseDaemon: 1. ? @ 0x0000ffff842f3830
2024.07.22 13:33:12.975067 [ 967 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.
2024.07.22 13:33:12.975103 [ 967 ] {} <Debug> SystemLogQueue (system.crash_log): Requested flush up to offset 1
2024.07.22 13:33:12.975113 [ 967 ] {} <Information> SentryWriter: Not sending crash report
2024.07.22 13:33:12.975120 [ 967 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues
2024.07.22 13:33:12.975123 [ 720 ] {} <Trace> SystemLog (system.crash_log): Flushing system log, 1 entries to flush up to offset 1
2024.07.22 13:33:12.975264 [ 967 ] {} <Fatal> BaseDaemon: No settings were changed
2024.07.22 13:33:12.975448 [ 720 ] {} <Debug> SystemLog (system.crash_log): Will use existing table system.crash_log for CrashLog
2024.07.22 13:33:12.975995 [ 720 ] {} <Trace> system.crash_log (f807ff07-fc07-4a1a-ad23-da79b5092491): Trying to reserve 1.00 MiB using storage policy from min volume index 0
2024.07.22 13:33:12.976025 [ 720 ] {} <Trace> DiskLocal: Reserved 1.00 MiB on local disk `default`, having unreserved 7.32 GiB.
2024.07.22 13:33:12.976515 [ 720 ] {} <Trace> MergedBlockOutputStream: filled checksums all_1_1_0 (state Temporary)
2024.07.22 13:33:12.976858 [ 720 ] {} <Trace> system.crash_log (f807ff07-fc07-4a1a-ad23-da79b5092491): Renaming temporary part tmp_insert_all_1_1_0 to all_3_3_0 with tid (1, 1, 00000000-0000-0000-0000-000000000000).
2024.07.22 13:33:12.977023 [ 720 ] {} <Trace> SystemLog (system.crash_log): Flushed system log up to offset 1
2024.07.22 13:33:12.987562 [ 619 ] {70dd095d-4079-463f-861e-b890cdc17fb5::202407_1_771_154} <Trace> system.metric_log (70dd095d-4079-463f-861e-b890cdc17fb5): Renaming temporary part tmp_merge_202407_1_771_154 to 202407_1_771_154 with tid (1, 1, 00000000-0000-0000-0000-000000000000).
2024.07.22 13:33:12.987657 [ 619 ] {70dd095d-4079-463f-861e-b890cdc17fb5::202407_1_771_154} <Trace> system.metric_log (70dd095d-4079-463f-861e-b890cdc17fb5) (MergerMutator): Merged 6 parts: [202407_1_766_153, 202407_771_771_0] -> 202407_1_771_154
2024.07.22 13:33:12.989643 [ 619 ] {} <Debug> MemoryTracker: Peak memory usage background process to apply mutate/merge in table: 62.42 MiB.
2024.07.22 13:33:12.997216 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Debug> executeQuery: Read 129 rows, 332.81 KiB in 0.033208 sec., 3884.606119007468 rows/sec., 9.79 MiB/sec.
2024.07.22 13:33:12.997472 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Debug> DynamicQueryHandler: Done processing query
2024.07.22 13:33:12.997489 [ 49 ] {b28c9b94-171c-48ba-bb00-fb90c84934fe} <Debug> MemoryTracker: Peak memory usage (for query): 3.49 MiB.
```

**Expected behavior**

I would expect the queries to succeed as they are official examples.

**Additional context**

m6g instance with 120GB+ assigned to the container.

Queries above don't need the AWS credentials, but of course, I have a role associated with the container that will require successful credentials fetch to access my S3 files.