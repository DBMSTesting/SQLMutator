ID: 7800
Title: Sizes of columns doesn't match encountered with Kafka under 19.17.2.4 
Description:
I've upgraded one of my QA clusters to 19.17.2.4, and I encountered the following errors with our kafka tables- which was blocking insertion of new records.

```
2019.11.15 13:13:57.813472 [ 36 ] {} <Error> virtual DB::UnionBlockInputStream::~UnionBlockInputStream(): Code: 9, e.displayText() = DB::Exception: Sizes of columns doesn't match: Timestamp: 65558, _timestamp: 0, Stack trace:

0. 0x561656a18c40 StackTrace::StackTrace() /usr/bin/clickhouse
1. 0x561656a18a15 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse
2. 0x561656709ae2 ? /usr/bin/clickhouse
3. 0x56165a67801e DB::KafkaBlockInputStream::readImpl() /usr/bin/clickhouse
4. 0x561659c6ebda DB::IBlockInputStream::read() /usr/bin/clickhouse
5. 0x561659dec682 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::loop(unsigned long) /usr/bin/clickhouse
6. 0x561659decd45 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /usr/bin/clickhouse
7. 0x561659ded6bd ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /usr/bin/clickhouse
8. 0x561656a631fc ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse
9. 0x56165c7e4f40 ? /usr/bin/clickhouse
10. 0x7f2dda4a2dd5 start_thread /usr/lib64/libpthread-2.17.so
11. 0x7f2dd9dbfead __clone /usr/lib64/libc-2.17.so
 (version 19.17.2.4 (official build))
2019.11.15 13:13:57.886841 [ 36 ] {} <Error> void DB::StorageKafka::threadFunc(): Code: 9, e.displayText() = DB::Exception: Sizes of columns doesn't match: Timestamp: 65542, _timestamp: 0, Stack trace:

0. 0x561656a18c40 StackTrace::StackTrace() /usr/bin/clickhouse
1. 0x561656a18a15 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse
2. 0x561656709ae2 ? /usr/bin/clickhouse
3. 0x56165a67801e DB::KafkaBlockInputStream::readImpl() /usr/bin/clickhouse
4. 0x561659c6ebda DB::IBlockInputStream::read() /usr/bin/clickhouse
5. 0x561659dec682 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::loop(unsigned long) /usr/bin/clickhouse
6. 0x561659decd45 DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /usr/bin/clickhouse
7. 0x561659ded6bd ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::UnionBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /usr/bin/clickhouse
8. 0x561656a631fc ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse
9. 0x56165c7e4f40 ? /usr/bin/clickhouse
10. 0x7f2dda4a2dd5 start_thread /usr/lib64/libpthread-2.17.so
11. 0x7f2dd9dbfead __clone /usr/lib64/libc-2.17.so
 (version 19.17.2.4 (official build))
```

The table schema is as follows:
```
CREATE TABLE default.histograms (`Timestamp` DateTime CODEC(DoubleDelta, LZ4), `Client` LowCardinality(String), `Path` LowCardinality(String), `Tags.Key` Array(LowCardinality(String)), `Tags.Value` Array(LowCardinality(String)) CODEC(LZ4HC(0)), `Range` Array(Float32), `Steps` Float32, `IndexNotation` Int32, `Histogram` Array(Float32)) ENGINE = ReplicatedMergeTree('/clickhouse/tables/histograms/{shard_qa}', '{replica_qa}') PARTITION BY toStartOfDay(Timestamp) ORDER BY (Path, Timestamp) TTL Timestamp + toIntervalDay(21) SETTINGS index_granularity = 8192

CREATE TABLE default.histograms_kafka (`Timestamp` DateTime CODEC(DoubleDelta), `Client` LowCardinality(String), `Path` LowCardinality(String), `Tags.Key` Array(LowCardinality(String)), `Tags.Value` Array(String), `Range` Array(Float32), `Steps` Float32, `IndexNotation` Int32, `Histogram` Array(Float32)) ENGINE = Kafka SETTINGS kafka_broker_list = 'SERVERS', kafka_topic_list = 'qa', kafka_format = 'JSONEachRow', kafka_group_name = 'qa', kafka_row_delimiter = '\n', kafka_num_consumers = 2, kafka_skip_broken_messages = 1
```