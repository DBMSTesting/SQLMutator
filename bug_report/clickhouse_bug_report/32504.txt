ID: 32504
Title: Insert to S3 with multipart upload to GCS fails + crashes clickhouse-server with SIGABRT
Description:
**Description**
Server crashes when using INSERT in S3 storage with gzip and error received from S3 provider.

Google Cloud Store has limitation for S3 API support.
E.g. it sends error if trying to use multipartUploads with Amazon S3 API.
This is Google limitation  due to it expects in all requests in multipart upload, including final request, to supply the same customer-supplied encryption key.
(https://cloud.google.com/storage/docs/migrating#methods-comparison) 

So it's ok that Clickhouse generates error when trying to insert large file to GCloud S3.
However not ok that once such error received, clickhouse server crashes with SIGABRT.
Also noticed that when using send w/o compression, only error generated and request rejected w/o crash.

**Version**
21.11.5.33

**How to reproduce**
- Create bucket in Google Cloud Store with service account and HMAC keys to support S3 API.
- Send large file so that multipart upload triggered:
```
    INSERT INTO FUNCTION s3('https://storage.googleapis.com/<bucket>/events.ndjson.gz', 
    '<key>', 
    '<secret>', 
    'JSONEachRow', 'ts_server DateTime, event_id String', 'gzip')
    SELECT
        now() ts_server,
        generateUUIDv4() event_id
    FROM system.numbers
    LIMIT 10000000;
```

**Expected behavior**
Prevent server crash on any error from S3 API, just reject the INSERT request.

**Additional desired behavior**
Provide ability to configure multipart upload disabling
 e.g. by setting s3_max_single_part_upload_size to some
 unbounded value e.g 0 or -1 or by using some other config

**Error message and/or stacktrace**
Sample logs attached: crash.log - when using gzip, error.log when using w/o gzip.
[logs.zip](https://github.com/ClickHouse/ClickHouse/files/7692298/logs.zip)
