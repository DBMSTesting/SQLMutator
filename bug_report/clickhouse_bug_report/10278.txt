ID: 10278
Title: Partitioning key overflow issues
Description:
**Describe the bug**

I am currently running ClickHouse version 19.5.3
I have 3 Distributed tables in 3 nodes. Each Distributed table points to a local MergeTree table.

I have a job which frequently write to a Distributed table of 1 instance. It is expected that after data is written to this Distributed table, based on sharding key, then data will be written to local MergeTree table or transferred to others MergeTree tables in 2 remaining nodes. 

But after running for a while, I checked the data in `/var/lib/clickhouse/data/database/my_distributed_table/` and found out that there are lots of data in a folder, which were supposed to be sent to another node . Seems like data were stuck to be distributed. 

I checked the log file and found this error

```
<Error> viki_user_data_pv_all.Distributed.DirectoryMonitor: Code: 246, e.displayText() = DB::Exception: Received from 10.20.0.12:19000. DB::Exception: Partition value mismatch between two parts with the same partition ID. Existing part: 21060207_1234714_1234714_0, newly added part: 21060207_1240624_1240624_0. Stack trace:

0. clickhouse-server(StackTrace::StackTrace()+0x16) [0x75aa876]
1. clickhouse-server(DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x22) [0x3593602]
2. clickhouse-server(DB::MergeTreeData::renameTempPartAndReplace(std::shared_ptr<DB::MergeTreeDataPart>&, SimpleIncrement*, DB::MergeTreeData::Transaction*, std::unique_lock<std::mutex>&, std::vector<std::shared_ptr<DB::MergeTreeDataPart const>, std::allocator<std::shared_ptr<DB::MergeTreeDataPart const> > >*)+0x1ca) [0x6c19f7a]
3. clickhouse-server(DB::MergeTreeData::renameTempPartAndReplace(std::shared_ptr<DB::MergeTreeDataPart>&, SimpleIncrement*, DB::MergeTreeData::Transaction*)+0x8c) [0x6c1acac]
4. clickhouse-server(DB::MergeTreeData::renameTempPartAndAdd(std::shared_ptr<DB::MergeTreeDataPart>&, SimpleIncrement*, DB::MergeTreeData::Transaction*)+0x41) [0x6c1add1]
5. clickhouse-server(DB::MergeTreeBlockOutputStream::write(DB::Block const&)+0x104) [0x6c0d984]
6. clickhouse-server(DB::PushingToViewsBlockOutputStream::write(DB::Block const&)+0x43) [0x6e6bab3]
7. clickhouse-server(DB::SquashingBlockOutputStream::writeSuffix()+0x13e) [0x6e7444e]
8. clickhouse-server(DB::TCPHandler::processInsertQuery(DB::Settings const&)+0x382) [0x35a23a2]
9. clickhouse-server(DB::TCPHandler::runImpl()+0x574) [0x35a29f4]
10. clickhouse-server(DB::TCPHandler::run()+0x2b) [0x35a395b]
11. clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x76d59df]
12. clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0x16a) [0x76d5dba]
13. clickhouse-server(Poco::PooledThread::run()+0x77) [0x77b1ce7]
14. clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x77ada38]
15. clickhouse-server() [0xb5f124f]
16. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db) [0x7fcf9713c6db]
17. /lib/x86_64-linux-gnu/libc.so.6(clone+0x3f) [0x7fcf966bb88f]
, Stack trace:

```

seem likes part `21060207_1234714_1234714_0` in the remote server 10.20.0.12 is somehow corrupted and then data can not be distributed to this server and got stuck at the server where data came in. 

In this scenario, should ClickHouse remove the corrupted data part?


