ID: 41754
Title: The "AzureBlobStorage" storage policy is not working
Description:
**Describe what's wrong**

The `AzureBlobStorage` storage policy is not working:
* when a table is defined with such storage policy & some inserts are done
  * blobs are uploaded to the blob storage
  * but they seem to be "truncated"
* and when a select query is done, then an error is raised (cf below)

**Does it reproduce on recent release?**

yes, it is present on release `22.8.4.7`

**How to reproduce**

* Define an `azure blob storage` storage policy as defined in the documentation
```
<storage_configuration>
    ...
    <disks>
        <blob_storage_disk>
            <type>azure_blob_storage</type>
            <storage_account_url>http://account.blob.core.windows.net</storage_account_url>
            <container_name>container</container_name>
            <account_name>account</account_name>
            <account_key>pass123</account_key>
            <metadata_path>/var/lib/clickhouse/disks/blob_storage_disk/</metadata_path>
            <cache_enabled>true</cache_enabled>
            <cache_path>/var/lib/clickhouse/disks/blob_storage_disk/cache/</cache_path>
            <skip_access_check>false</skip_access_check>
        </blob_storage_disk>
    </disks>
    ...
</storage_configuration>
```
* create the following table
```
CREATE TABLE default.test_n
(
    `n` UInt32
)
ENGINE = MergeTree
ORDER BY n
SETTINGS index_granularity = 8192, storage_policy = '<my_storage_policy>'
```
* generate some data to be inserted through
```
clickhouse-client -q "select rand() as n from numbers(800000) format CSV" > ./n_800K.csv
```
(800K rows so around 3.2MB file)
* insert the data into the table
```
INSERT INTO default.test_n from infile 'n_800K.csv' format CSV
```
* the insert finished, then a `select count(*) from test_n` is working....
* but a `select * from test_n limit 3` is not working....such kind of errors are raised
```
Received exception from server (version 22.8.4):
Code: 432. DB::Exception: Received from localhost:9000. DB::Exception: Unknown codec family code: 134: (while reading column n): While executing MergeTreeInOrder. (UNKNOWN_CODEC)
```

* when looking at what was uploaded to the blobstore, when i look at the blob related to the `data.bin` file for the part generated by my insert, i can see that the size of the blob is not the expected one
  * the blob should be around 3MB....but it is rather around few hundreds KB
  * when i do the exact same test but with an S3 storage policy and look at the uploaded blob, i can see a 3MB blob for my upload
  * on Azure the blob related to the data looks "truncated"  (cf below for my analysis on the root cause of this issue)

The version of clickhouse used is `22.8.4.7`

**Expected behavior**

The insert should work and you should be able to "query" the inserted data without any error.


**Additional context**

When looking at the code in charge of the upload to the Azure blob storage within `WriteBufferFromAzureBlobStorage`, i've seen the following that, to me, seems to be the root cause of this issue:
* within `nextImpl` (https://github.com/ClickHouse/ClickHouse/blob/v22.8.4.7-lts/src/Disks/IO/WriteBufferFromAzureBlobStorage.cpp#L84-L89), we are both doing a `stageBlock` and then just after that a `commitBlockList`
* my understanding is that this `nextImpl` is called by chunk of 1MB by default (`DBMS_DEFAULT_BUFFER_SIZE`)
* so really looks like we are uploading (stageBlobk) and committing (commitBlockList) chunk of 1MB....so we are in a situation where for every chunk of 1MB a new blob of 1MB is uploaded .... overwriting the previous one generated with the previous chunk of data
* so in my use case where i should upload a blob of 3.2MB, a first blob of 1MB is uploaded, then the 2ndMB are uploaded and overriding the previous 1MB...and so on until the end ... and at the end of the insert only remains a blob of 200KB
  * which then lead to the issue when i do my select

It looks like that the `commitBlockList` call should rather be done within `finalizeImpl` rather than `nextImpl`, no?
(it is actually what was done in the first version of the PR introducing the feature -> https://github.com/ClickHouse/ClickHouse/pull/31505/files#diff-1fe8880bb294ceca7ed96969a660163121df287348d242e143a5842ba7f3be3aR61)

The last point i am thinking about is, in case the issue is confirmed and some modifications have to be done around this `WriteBufferFromAzureBlobStorage`:
* currently, looks like we are uploading `blocks` of 1MB (through stageBlock)
* it may be important for performance reason to follow the same kind of strategy as on `WriteBufferFromS3` where those chunks of 1MB managed by the write buffer are accumulated in memory and uploaded only when something like few 100s of MBs are reached (so instead of uploading "multiparts" of 1MB, we rather upload multi parts of "100MB")
* also looks like on `WriteBufferFromS3` there are some parallelism in place when uploading the "multi parts" (through the `threadPoolWriter` defined in IObjectStorage)....it may also be good to have the same "mechanism" on the Azure blob storage as well