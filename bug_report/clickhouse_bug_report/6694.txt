ID: 6694
Title: Allocated RAM keeps increasing when exporting data (into a RowBinary-table)
Description:
Hi

I'm doing a lot of data exports (pls. don't challenge me on the "why" - I just have to, to test some silly theories).
I do the data exports for example like this:
`create table test_exported engine=File(RowBinary) as select col1 from test order by col1, col2, col3;`

**Problem**
When the source table ("test" in the above example) is small (e.g. 2 billion rows) I can export all columns (e.g. "col1" in the above example, then I would export "col2", and so on...) in a single sequence (concurrently or serially), but when the source table becomes bigger I have to split the exports into smaller groups and inbetween **I have to stop and then restart clickhouse** otherwise I run out of RAM and the PC starts swapping.

The RAM is not freed once an export finishes, nor when I drop the table created by the export-SQL.
Currently for me (with my 32GBs RAM) the limit for a single export is probably with a source table of ~10 billion rows (yesterday I got very near to that after having run concurrent 2 exports where each one queried a source table that contained 5 billion rows).

**Infos**
The "profile" (in users.xml) that I use for the user who runs such exports-SQLs contains the these settings:
```
<max_query_size>1000000</max_query_size>

<max_memory_usage>10000000000</max_memory_usage>
<max_bytes_before_external_sort>1000000000</max_bytes_before_external_sort>
<max_bytes_before_external_group_by>1000000000</max_bytes_before_external_group_by>

<max_partitions_per_insert_block>0</max_partitions_per_insert_block>

<use_uncompressed_cache>0</use_uncompressed_cache>
<load_balancing>random</load_balancing>
<readonly>0</readonly>
<background_pool_size>16</background_pool_size>

<max_execution_time>999999999</max_execution_time>
<receive_timeout>999999999</receive_timeout>
<send_timeout>999999999</send_timeout>
```

The file "config.xml" contains these RAM-relevant settings...
```
<uncompressed_cache_size>1073741824</uncompressed_cache_size>
<mark_cache_size>5368709120</mark_cache_size>
```
..., the "tmp_path"-parameter points to a directory mounted on an SSD formatted with EXT4 and the "<path>"-parameter points to another directory mounted on a HDD formatted as well with EXT4.

With these parameters I see that while the SQL is running it creates many temporary files of ~435MBs in the "<tmp_path>"-directory and that at the end it reads from them creating the giant final table and then the temp-files are deleted => everything ok here (no error messages in "/var/log/clickhouse-server/clickhouse-server.log", the issue is really just the amount of allocated RAM that keeps increasing.
Tested with 18.12.17 + 19.5.3.8 + 19.9.5.36 (if I remember correctly - I'm currently on 19.5.3.8).

**Question**
Is this supposed to happen?
Am I missing some parameter to free up RAM and/or limit RAM allocation?
Thank you

(EDIT: trying to create a testscript)