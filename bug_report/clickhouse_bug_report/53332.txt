ID: 53332
Title: Getting value is too big to apply padding exception while ingesting JSON from S3
Description:
I have a CH cluster deployed on AWS EKS with 16GB memory nodes. I frequently insert data to Clickhouse with command:
```
INSERT INTO test_table
SELECT * 
FROM s3('https://bucket/prefix/**', 'JSONEachRow')
SETTINGS max_threads = 8, s3_skip_empty_files = 1
```
CH version:  23.6.2.18

Iâ€™m getting
```
Code: 69. DB::Exception: Received from localhost:9000. DB::Exception: value is too big to apply padding: While executing ParallelParsingBlockInputFormat: While executing S3. (ARGUMENT_OUT_OF_BOUND)
```
errors consistently when trying to download a batch of files from s3. Batch size is around 1MB gzip compressed JSON. Is there a setting on clickhouse side where i can correlate the max batch size i need to form without hitting this limit? Compression ratio is high so 100KB is 3.5MB uncompressed. This does not happen when I try with smaller batch size. 
#CH version:  23.6.2.18
The same files are ok when I set input_format_parallel_parsing=0 during query time. I was not sure if I should open a bug or if this is a misconfiguration  so any help is appreciated. 