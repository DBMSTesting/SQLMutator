ID: 41365
Title: Crash from DB::VolumeJBOD::reserve 
Description:
```
2022.09.15 11:21:25.116969 [ 428 ] {} <Fatal> BaseDaemon: ########################################
2022.09.15 11:21:25.117020 [ 428 ] {} <Fatal> BaseDaemon: (version 22.6.6.16 (official build), build id: A2D76F3CFE3C4089) (from thread 214) (no query) Received signal Segmentation fault (11)
2022.09.15 11:21:25.117050 [ 428 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Address not mapped to object.
2022.09.15 11:21:25.117791 [ 428 ] {} <Fatal> BaseDaemon: Stack trace: 0x15cac4e6 0x15ca467d 0x16ca3754 0x16bdb0c5 0x16bdab7f 0x16acb735 0x155686d8 0x1556b996 0x1556c80e 0xb94c697 0xb94fabd 0x7fab5e136609 0x7fab5e05b133
2022.09.15 11:21:25.121241 [ 428 ] {} <Fatal> BaseDaemon: 2. DB::VolumeJBOD::reserve(unsigned long) @ 0x15cac4e6 in /usr/bin/clickhouse
2022.09.15 11:21:25.121283 [ 428 ] {} <Fatal> BaseDaemon: 3. DB::StoragePolicy::reserve(unsigned long, unsigned long) const @ 0x15ca467d in /usr/bin/clickhouse
2022.09.15 11:21:25.128191 [ 428 ] {} <Fatal> BaseDaemon: 4. DB::MergeTreePartsMover::selectPartsForMove(std::__1::vector<DB::MergeTreeMoveEntry, std::__1::allocator<DB::MergeTreeMoveEntry> >&, std::__1::function<bool (std::__1::share
d_ptr<DB::IMergeTreeDataPart const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)> const&, std::__1::lock_guard<std::__1::mutex> const&) @ 0x16ca3754 in /usr/bin/clickhouse
2022.09.15 11:21:25.128234 [ 428 ] {} <Fatal> BaseDaemon: 5. DB::MergeTreeData::selectPartsForMove() @ 0x16bdb0c5 in /usr/bin/clickhouse
2022.09.15 11:21:25.128256 [ 428 ] {} <Fatal> BaseDaemon: 6. DB::MergeTreeData::scheduleDataMovingJob(DB::BackgroundJobsAssignee&) @ 0x16bdab7f in /usr/bin/clickhouse
2022.09.15 11:21:25.128282 [ 428 ] {} <Fatal> BaseDaemon: 7. DB::BackgroundJobsAssignee::threadFunc() @ 0x16acb735 in /usr/bin/clickhouse
2022.09.15 11:21:25.128298 [ 428 ] {} <Fatal> BaseDaemon: 8. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x155686d8 in /usr/bin/clickhouse
2022.09.15 11:21:25.128315 [ 428 ] {} <Fatal> BaseDaemon: 9. DB::BackgroundSchedulePool::threadFunction() @ 0x1556b996 in /usr/bin/clickhouse
2022.09.15 11:21:25.128330 [ 428 ] {} <Fatal> BaseDaemon: 10. ? @ 0x1556c80e in /usr/bin/clickhouse
2022.09.15 11:21:25.128350 [ 428 ] {} <Fatal> BaseDaemon: 11. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94c697 in /usr/bin/clickhouse
2022.09.15 11:21:25.128364 [ 428 ] {} <Fatal> BaseDaemon: 12. ? @ 0xb94fabd in /usr/bin/clickhouse
2022.09.15 11:21:25.128380 [ 428 ] {} <Fatal> BaseDaemon: 13. ? @ 0x7fab5e136609 in ?
2022.09.15 11:21:25.128408 [ 428 ] {} <Fatal> BaseDaemon: 14. clone @ 0x7fab5e05b133 in ?
2022.09.15 11:21:25.285938 [ 428 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 4047F5140528A7E5E7529C5B1D0E9C2C)

.... 2022.09.15 11:21:30.389499 [ 24 ] {} <Warning> data.search_api: Would like to reserve space on disk 'gp3max' by TTL rule of table 'data.search_api' but there is not enough space
```

```xml
<yandex>
    <storage_configuration>
        <disks>
            <default>
                <keep_free_space_bytes>1024</keep_free_space_bytes>
            </default>
            <gp3max>
                <path>/mnt/gp3max/</path>
            </gp3max>
            <hdd>
                <path>/mnt/hdd/</path>
            </hdd>
            <hdd2>
                <path>/mnt/hdd2/</path>
            </hdd2>
            <hddcold>
                <path>/mnt/hddcold/</path>
            </hddcold>
        </disks>

        <policies>
            <ebs_hot_and_cold>
                <volumes>
                    <gp3max_volume>
                        <disk>gp3max</disk>
                    </gp3max_volume>
                    <hdd_volume>
                        <disk>hdd</disk>
                        <disk>hdd2</disk>
                        <load_balancing>least_used</load_balancing>
                    </hdd_volume>
                    <hddcold_volume>
                        <disk>hddcold</disk>
                    </hddcold_volume>
                </volumes>
            </ebs_hot_and_cold>
           <ebs_gp3max_and_hdd>
                <volumes>
                    <gp3max_volume>
                        <disk>gp3max</disk>
                    </gp3max_volume>
                    <hdd_volume>
                        <disk>hdd</disk>
                        <disk>hdd2</disk>
                    </hdd_volume>
                    <hddcold_volume>
                        <disk>hddcold</disk>
                    </hddcold_volume>

                </volumes>
            </ebs_gp3max_and_hdd>
        </policies>
    </storage_configuration>
```