ID: 9270
Title: dictionary - DB::Exception: Allocator: Cannot mmap 6.21 EiB.
Description:
Hi team.

I`m trying to using dictionary but I got a Allocator exception.
There are only 30 rows in tables. And I think my memory is enough.

```sql
SELECT 
    dictGet('test.table1_dict', 'col6', (col1, col2, col3, col4, col5)), 
    dictGet('test.table1_dict', 'col7', (col1, col2, col3, col4, col5)), 
    dictGet('test.table1_dict', 'col8', (col1, col2, col3, col4, col5)), 
    dictGet('test.table1_dict', 'col9', (col1, col2, col3, col4, col5))
FROM table1
WHERE dictHas('test.table1_dict', (col1, col2, col3, col4, col5))

Received exception from server (version 20.1.4):
Code: 173. DB::Exception: Received from localhost:9000. DB::Exception: Allocator: Cannot mmap 6.21 EiB., errno: 12, strerror: Cannot allocate memory. 

0 rows in set. Elapsed: 0.009 sec. 
```

CH version - 20.1.4 revision 54431.

Memory informations are like below.
```sql
SELECT *
FROM system.settings
WHERE name LIKE '%memory%'

┌─name───────────────────────────────────────┬─value───────┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐
│ distributed_aggregation_memory_efficient   │ 0           │       0 │ Is the memory-saving mode of distributed aggregation enabled.                                                                                                             │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ aggregation_memory_efficient_merge_threads │ 0           │       0 │ Number of threads to use for merge intermediate aggregation results in memory efficient mode. When bigger, then more memory is consumed. 0 means - same as 'max_threads'. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ memory_tracker_fault_probability           │ 0           │       0 │ For testing of `exception safety` - throw an exception every time you allocate memory with the specified probability.                                                     │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ max_memory_usage                           │ 10000000000 │       1 │ Maximum memory usage for processing of single query. Zero means unlimited.                                                                                                │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ max_memory_usage_for_user                  │ 0           │       0 │ Maximum memory usage for processing all concurrently running queries for the user. Zero means unlimited.                                                                  │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ max_memory_usage_for_all_queries           │ 0           │       0 │ Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited.                                                                 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
└────────────────────────────────────────────┴─────────────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘

6 rows in set. Elapsed: 0.008 sec. 

SELECT *
FROM system.settings
WHERE name LIKE '%cache%'

┌─name──────────────────────────────┬─value──────┬─changed─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐
│ use_uncompressed_cache            │ 0          │       1 │ Whether to use the cache of uncompressed blocks.                                                                                                                               │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ merge_tree_max_rows_to_use_cache  │ 1048576    │       0 │ The maximum number of rows per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.) │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ merge_tree_max_bytes_to_use_cache │ 2013265920 │       0 │ The maximum number of rows per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.) │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
│ mark_cache_min_lifetime           │ 0          │       0 │ Obsolete setting, does nothing. Will be removed after 2020-05-31                                                                                                               │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │        0 │
└───────────────────────────────────┴────────────┴─────────┴───────────────────────────────────────────────
```

config.xml & users.xml
```xml
<mark_cache_size>5368709120</mark_cache_size>
...
<max_memory_usage>10000000000</max_memory_usage>
<max_bytes_before_external_group_by>3000000000</max_bytes_before_external_group_by>
<http_max_multipart_form_data_size>3000000000</http_max_multipart_form_data_size>
```

```bash
# free -h
              total        used        free      shared  buff/cache   available
Mem:            15G        877M         11G         40M        2.7G         14G
Swap:          7.9G          0B        7.9G

# cat /proc/sys/vm/overcommit_memory
0
```

This is my test query
```sql
CREATE TABLE table1
(
    `col1` String, 
    `col2` Int16, 
    `col3` String, 
    `col4` Int32, 
    `col5` String, 
    `col6` Nullable(Float64), 
    `col7` Nullable(Float64), 
    `col8` Nullable(DateTime), 
    `col9` Nullable(String), 
    `col10` Nullable(String), 
    `col11` Nullable(String), 
    `col12` Nullable(String), 
    `col13` Nullable(Int32), 
    `col14` Nullable(DateTime), 
    `col15` Nullable(DateTime), 
    `col16` Nullable(DateTime), 
    `col17` Nullable(DateTime), 
    `col18` Nullable(DateTime), 
    `col19` Nullable(DateTime), 
    `col20` Nullable(String)
)
ENGINE = MergeTree
ORDER BY (col1, col2, col3, col4, col5);

# cat sample.csv | clickhouse-client --database=test --query="insert into test.table1 format CSV"

CREATE DICTIONARY table1_dict
(
 col1 String,
 col2 Int16,
 col3 String,
 col4 Int32,
 col5 String,
 col6 Float64,
 col7 Float64,
 col8 DateTime,
 col9 String,
 col10 String,
 col11 String,
 col12 String,
 col13 Int32,
 col14 DateTime,
 col15 DateTime,
 col16 DateTime,
 col17 DateTime,
 col18 DateTime,
 col19 DateTime,
 col20 String
)
PRIMARY KEY col1,col2,col3,col4,col5
SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE table1 DB 'test' USER 'default'))
LIFETIME(MIN 0 MAX 0) LAYOUT(COMPLEX_KEY_HASHED());
```

sample.csv
```csv
"id1",1,"20200127-1",701,"20200127-1-01",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:37:59",\N,"2020-02-04 11:35:14","2020-02-08 05:32:04",\N,\N,"12345"
"id1",1,"20200127-1",702,"20200127-1-02",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:37:59",\N,"2020-02-04 11:35:14","2020-02-08 05:32:04",\N,\N,"12345"
"id1",1,"20200127-1",703,"20200127-1-03",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:37:59",\N,"2020-02-04 11:35:14","2020-02-08 05:32:04",\N,\N,"12345"
"id1",1,"20200127-1",704,"20200127-1-04",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:37:59",\N,"2020-02-04 11:35:14","2020-02-08 05:32:04",\N,\N,"12345"
"id1",1,"20200127-1",705,"20200127-1-05",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:37:59",\N,"2020-02-04 11:35:14","2020-02-08 05:32:04",\N,\N,"12345"
"id1",1,"20200202-1",711,"20200202-1-01",0,200,\N,"C2","Hello","C40",\N,1,"2020-02-03 11:07:57",\N,\N,\N,"2020-02-03 11:09:23",\N,\N
"id1",1,"20200202-2",712,"20200202-2-01",0,0,\N,"C3","bye","R40",\N,1,"2020-02-03 14:13:10",\N,"2020-02-03 16:11:31","2020-02-07 05:32:05","2020-02-07 11:18:15","2020-02-07 11:18:16","123455"
"id1",1,"20200202-2",713,"20200202-2-02",0,0,\N,"C3","bye","R40",\N,1,"2020-02-03 14:13:10",\N,"2020-02-03 16:11:31","2020-02-07 05:32:05","2020-02-07 11:18:15","2020-02-07 11:18:16","123455"
"id1",2,"20200128-1",701,"20200128-1-01",0,0,\N,"N1","Hi","N40",\N,2,"2020-02-03 17:07:27",\N,"2020-02-05 13:33:55","2020-02-13 05:32:04",\N,\N,"A123755"
"id1",2,"20200131-1",701,"20200131-1-01",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 13:07:17",\N,"2020-02-04 13:47:55","2020-02-12 05:32:04",\N,\N,"A123485"
"id1",2,"20200201-1",701,"20200201-1-01",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 21:07:37",\N,"2020-02-05 13:40:51","2020-02-13 05:32:04",\N,\N,"A123455"
"id1",2,"20200202-1",711,"20200202-1-01",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 02:06:54",\N,"2020-02-04 13:36:45","2020-02-12 05:32:04",\N,\N,"A123459"
"id1",2,"20200202-1",712,"20200202-1-02",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 02:06:54",\N,"2020-02-04 13:36:45","2020-02-12 05:32:04",\N,\N,"A123429"
"id2",1,"20200131-1",401,"20200131-1-01",0,210,"2020-02-16 05:22:04","N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-05 17:30:05","2020-02-09 05:32:05",\N,\N,"454545"
"id2",1,"20200131-1",402,"20200131-1-02",0,210,"2020-02-16 05:22:04","N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-05 17:30:05","2020-02-09 05:32:05",\N,\N,"454545"
"id2",1,"20200131-1",403,"20200131-1-03",0,270,"2020-02-16 05:22:04","N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-05 17:30:05","2020-02-09 05:32:05",\N,\N,"454545"
"id2",1,"20200131-1",404,"20200131-1-04",0,270,"2020-02-16 05:22:04","N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-05 17:30:05","2020-02-09 05:32:05",\N,\N,"454545"
"id2",1,"20200131-1",405,"20200131-1-05",0,380,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-11 16:52:58","2020-02-15 05:32:04",\N,\N,"6892144935823"
"id2",1,"20200131-1",406,"20200131-1-06",0,380,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-11 16:52:58","2020-02-15 05:32:04",\N,\N,"6892144935823"
"id2",1,"20200131-1",407,"20200131-1-07",0,280,\N,"C2","Hello","C40",\N,1,"2020-02-03 10:11:00",\N,\N,\N,"2020-02-04 11:01:21",\N,\N
"id2",1,"20200131-1",408,"20200131-1-08",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:11:00",\N,"2020-02-05 17:30:05","2020-02-09 05:32:04",\N,\N,"454545"
"id2",1,"20200201-1",401,"20200201-1-01",0,190,"2020-02-16 05:22:05","N1","Hi","N40",\N,1,"2020-02-03 12:06:17",\N,"2020-02-05 17:30:30","2020-02-09 05:32:03",\N,\N,"90071"
"id2",1,"20200201-1",402,"20200201-1-01",0,160,"2020-02-14 05:22:13","N1","Hi","N40",\N,1,"2020-02-03 06:21:05",\N,"2020-02-03 17:42:35","2020-02-07 05:32:04",\N,\N,"96575"
"id2",1,"20200201-1",403,"20200201-1-02",0,230,"2020-02-14 05:22:13","N1","Hi","N40",\N,1,"2020-02-03 06:21:05",\N,"2020-02-03 17:42:35","2020-02-07 05:32:04",\N,\N,"96575"
"id2",1,"20200202-1",404,"20200202-1-01",0,130,"2020-02-14 05:22:14","N1","Hi","N40",\N,1,"2020-02-03 14:00:39",\N,"2020-02-03 17:42:45","2020-02-07 05:32:04",\N,\N,"96850"
"id3",1,"20200130-1",391,"20200130-1-01",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:26:46",\N,"2020-02-05 15:33:01","2020-02-08 05:32:05",\N,\N,"27243"
"id3",1,"20200130-1",392,"20200130-1-02",0,300,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:26:46",\N,"2020-02-10 16:16:11","2020-02-13 05:32:06",\N,\N,"92512"
"id3",1,"20200131-1",393,"20200131-1-01",0,0,\N,"C2","Hello","C40",\N,1,"2020-02-03 10:24:38",\N,\N,\N,"2020-02-05 14:04:40",\N,\N
"id3",1,"20200131-2",391,"20200131-1-01",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:22:08",\N,"2020-02-06 14:27:06","2020-02-09 05:32:04",\N,\N,"46433"
"id3",1,"20200131-2",392,"20200131-1-02",0,0,\N,"N1","Hi","N40",\N,1,"2020-02-03 10:22:08",\N,"2020-02-06 14:27:06","2020-02-09 05:32:02",\N,\N,"46433"
```