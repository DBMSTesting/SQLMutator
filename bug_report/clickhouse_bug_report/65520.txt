ID: 65520
Title: Insertion into distributed table causes Segmentation fault
Description:
**Describe what's wrong**
Insertion into distributed table causes Segmentation fault

**crash reports**
```
SentryWriter: Sending crash reports is initialized with https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277 endpoint and /var/lib/clickhouse/tmp/sentry temp folder
```

**How to reproduce**

* Which ClickHouse server version to use: `24.3.3.102`
* Which interface to use, if it matters: `MySQL Shell`
* Non-default settings, if any: `distributed_product_mode='allow'`

* init database:
```sql
create database ch_main on cluster default;
use ch_main;

create table __t_m on cluster default ( 
  c_mpcnr33 Int32 primary key ,
  c_v8s String ,
  c_l Int32 ,
  c_jismi1 String ,
  c_p37t64z75 Bool not null ,
  c_uz Bool ,
  c_rp Int32 primary key ,
  c_d56dwp13jp Bool ,
  c_sf__xnd4 Float64 not null ,
  );

-- sql #155
create table t_m on cluster default as __t_m ENGINE = Distributed(default, ch_main, __t_m, c_mpcnr33);

create table __t_nh1w on cluster default ( 
  c_sfdzg Int32 ,
  c_xf Bool ,
  c_u3xs92nr4c String ,
  c_b_m Int32 primary key ,
  c_lgy Int32 ,
  );

-- sql #101
create table t_nh1w on cluster default as __t_nh1w ENGINE = Distributed(default, ch_main, __t_nh1w, c_b_m);



```

* Queries to run that lead to an unexpected result
```sql
insert into t_m (c_mpcnr33, c_v8s, c_l, c_jismi1, c_p37t64z75, c_uz, c_rp, c_d56dwp13jp, c_sf__xnd4) values 
(868701807, coalesce((select c_u3xs92nr4c from t_nh1w order by c_u3xs92nr4c limit 1 offset 6)
  , 'llwlzwb3'), 1824351772, coalesce(MACNumToString(lcm(-3, -6)), 'f')) 
;
```

clickhouse log:
```
2024.06.21 08:13:06.769919 [ 39 ] {mysql:3:18d1ab63-3d84-4578-b99e-ec4e9474a4e8} <Debug> executeQuery: (from 10.0.7.254:48550) insert into t_m (c_mpcnr33, c_v8s, c_l, c_jismi1, c_p37t64z75, c_uz, c_rp, c_d56dwp13jp, c_sf__xnd4) values  (stage: Complete)
2024.06.21 08:13:06.772360 [ 719 ] {} <Fatal> BaseDaemon: ########## Short fault info ############
2024.06.21 08:13:06.772450 [ 719 ] {} <Fatal> BaseDaemon: (version 24.3.3.102 (official build), build id: EF9E1BD0781C858153E899F2D95A044F4DD82F9B, git hash: 7e7f3bdd9be3ced03925d1d602037db8687e6401) (from thread 39) Received signal 11
2024.06.21 08:13:06.772499 [ 719 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault
2024.06.21 08:13:06.772527 [ 719 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Address not mapped to object.
2024.06.21 08:13:06.772553 [ 719 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000000010c33c2b
2024.06.21 08:13:06.772574 [ 719 ] {} <Fatal> BaseDaemon: ########################################
2024.06.21 08:13:06.772595 [ 719 ] {} <Fatal> BaseDaemon: (version 24.3.3.102 (official build), build id: EF9E1BD0781C858153E899F2D95A044F4DD82F9B, git hash: 7e7f3bdd9be3ced03925d1d602037db8687e6401) (from thread 39) (query_id: mysql:3:18d1ab63-3d84-4578-b99e-ec4e9474a4e8) (query: insert into t_m (c_mpcnr33, c_v8s, c_l, c_jismi1, c_p37t64z75, c_uz, c_rp, c_d56dwp13jp, c_sf__xnd4) values 
) Received signal Segmentation fault (11)
2024.06.21 08:13:06.772628 [ 719 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Address not mapped to object.
2024.06.21 08:13:06.772647 [ 719 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000000010c33c2b
2024.06.21 08:13:06.772758 [ 719 ] {} <Fatal> BaseDaemon: 2. DB::QueryNode const& typeid_cast<DB::QueryNode const&, DB::IQueryTreeNode>(DB::IQueryTreeNode&) @ 0x0000000010c33c2b
2024.06.21 08:13:06.901022 [ 719 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: F3691ADAC982D94BEEF75EF7C8E42F75)
2024.06.21 08:13:06.901121 [ 719 ] {} <Debug> SystemLogQueue (system.crash_log): Requested flush up to offset 1
2024.06.21 08:13:06.901201 [ 678 ] {} <Debug> SystemLog (system.crash_log): Creating new table system.crash_log for CrashLog
2024.06.21 08:13:06.901296 [ 719 ] {} <Information> SentryWriter: Sending crash report
2024.06.21 08:13:06.906809 [ 678 ] {} <Debug> system.crash_log (c712639d-59b1-4cee-8f1d-8933b6b43190): Loading data parts
2024.06.21 08:13:06.907068 [ 678 ] {} <Debug> system.crash_log (c712639d-59b1-4cee-8f1d-8933b6b43190): There are no data parts
2024.06.21 08:13:07.642377 [ 185 ] {} <Debug> DNSResolver: Updating DNS cache
2024.06.21 08:13:07.643072 [ 185 ] {} <Debug> DNSResolver: Updated DNS cache
2024.06.21 08:13:08.237043 [ 719 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues
2024.06.21 08:13:08.237185 [ 719 ] {} <Fatal> BaseDaemon: Changed settings: use_uncompressed_cache = false, load_balancing = 'in_order', log_queries = true, distributed_product_mode = 'allow', prefer_column_name_to_alias = true, max_memory_usage = 10000000000, use_structure_from_insertion_table_in_table_functions = 0
2024.06.21 08:13:21.854053 [ 1 ] {} <Information> SentryWriter: Sending crash reports is initialized with https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277 endpoint and /var/lib/clickhouse/tmp/sentry temp folder
2024.06.21 08:13:21.906566 [ 1 ] {} <Information> Application: Starting ClickHouse 24.3.3.102 (revision: 54484, git hash: 7e7f3bdd9be3ced03925d1d602037db8687e6401, build id: EF9E1BD0781C858153E899F2D95A044F4DD82F9B), PID 1
2024.06.21 08:13:21.906686 [ 1 ] {} <Information> Application: starting up
2024.06.21 08:13:21.906697 [ 1 ] {} <Information> Application: OS name: Linux, version: 6.5.0-28-generic, architecture: x86_64
2024.06.21 08:13:21.913999 [ 1 ] {} <Information> Application: Available RAM: 503.52 GiB; physical cores: 64; logical cores: 128.
2024.06.21 08:13:21.914020 [ 1 ] {} <Information> Application: Available CPU instruction sets: SSE, SSE2, SSE3, SSSE3, SSE41, SSE42, F16C, POPCNT, BMI1, BMI2, PCLMUL, AES, AVX, FMA, AVX2, SHA, ADX, RDRAND, RDSEED, RDTSCP, CLFLUSHOPT, CLWB, XSAVE, OSXSAVE
```

**Expected behavior**
No crash.

**Additional context**
docker compose config:
```
version: '3.8'
services:
  clickhouse-01:
    image: "clickhouse/clickhouse-server:${CHVER:-latest}"
    user: "101:101"
    container_name: clickhouse-01
    hostname: clickhouse-01
    networks:
      cluster_2S_1R:
        ipv4_address: 10.0.7.1
    volumes:
      - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml
      - ${PWD}/fs/volumes/clickhouse-01/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml
    depends_on:
      - clickhouse-keeper-01
      - clickhouse-keeper-02
      - clickhouse-keeper-03
  clickhouse-02:
    image: "clickhouse/clickhouse-server:${CHVER:-latest}"
    user: "101:101"
    container_name: clickhouse-02
    hostname: clickhouse-02
    networks:
      cluster_2S_1R:
        ipv4_address: 10.0.7.2
    volumes:
      - ${PWD}/fs/volumes/clickhouse-02/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml
      - ${PWD}/fs/volumes/clickhouse-02/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml
    depends_on:
      - clickhouse-keeper-01
      - clickhouse-keeper-02
      - clickhouse-keeper-03
  clickhouse-keeper-01:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "101:101"
    container_name: clickhouse-keeper-01
    hostname: clickhouse-keeper-01
    networks:
      cluster_2S_1R:
        ipv4_address: 10.0.7.5
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-01/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml

  clickhouse-keeper-02:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "101:101"
    container_name: clickhouse-keeper-02
    hostname: clickhouse-keeper-02
    networks:
      cluster_2S_1R:
        ipv4_address: 10.0.7.6
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-02/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml

  clickhouse-keeper-03:
    image: "clickhouse/clickhouse-keeper:${CHKVER:-latest-alpine}"
    user: "101:101"
    container_name: clickhouse-keeper-03
    hostname: clickhouse-keeper-03
    networks:
      cluster_2S_1R:
        ipv4_address: 10.0.7.7
    volumes:
      - ${PWD}/fs/volumes/clickhouse-keeper-03/etc/clickhouse-keeper/keeper_config.xml:/etc/clickhouse-keeper/keeper_config.xml

networks:
  cluster_2S_1R:
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.7.0/24
          gateway: 10.0.7.254
```

config.xml
```xml
<clickhouse replace="true">
    <logger>
        <level>debug</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>3</count>
    </logger>
    <display_name>cluster_2S_1R node 1</display_name>
    <listen_host>0.0.0.0</listen_host>
    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>
    <mysql_port>9004</mysql_port>
    <postgresql_port>9005</postgresql_port>
    <user_directories>
        <users_xml>
            <path>users.xml</path>
        </users_xml>
        <local_directory>
            <path>/var/lib/clickhouse/access/</path>
        </local_directory>
    </user_directories>
    <distributed_ddl>
        <path>/clickhouse/task_queue/ddl</path>
    </distributed_ddl>
    <remote_servers>
        <default>
            <shard>
                <replica>
                    <host>clickhouse-01</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <replica>
                    <host>clickhouse-02</host>
                    <port>9000</port>
                </replica>
            </shard>
        </default>
    </remote_servers>
    <zookeeper>
        <node>
            <host>clickhouse-keeper-01</host>
            <port>9181</port>
        </node>
        <node>
            <host>clickhouse-keeper-02</host>
            <port>9181</port>
        </node>
        <node>
            <host>clickhouse-keeper-03</host>
            <port>9181</port>
        </node>
    </zookeeper>
    <macros>
        <shard>01</shard>
        <replica>01</replica>
    </macros>
</clickhouse>
```
users.xml
```xml
<?xml version="1.0"?>
<clickhouse replace="true">
    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <use_uncompressed_cache>0</use_uncompressed_cache>
            <load_balancing>in_order</load_balancing>
            <log_queries>1</log_queries>
            <distributed_product_mode>allow</distributed_product_mode>
        </default>
    </profiles>
    <users>
        <default>
            <access_management>1</access_management>
            <profile>default</profile>
            <networks>
                <ip>::/0</ip>
            </networks>
            <password></password>
            <quota>default</quota>
            <access_management>1</access_management>
            <named_collection_control>1</named_collection_control>
            <show_named_collections>1</show_named_collections>
            <show_named_collections_secrets>1</show_named_collections_secrets>
        </default>
    </users>
    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

#### about us
We are the BASS team from the School of Cyber Science and Technology at Beihang University. Our main focus is on system software security, operating systems, and program analysis research, as well as the development of automated program testing frameworks for detecting software defects. Using our self-developed database vulnerability testing tool, we have identified the potential above-mentioned vulnerability that may lead to database logic error.