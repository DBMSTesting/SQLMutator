ID: 1431
Title: [Bug]: Spark JDBC通过官方驱动读取的int类型的时候，会报 Out of range value for column
Description:
测试表结构

```sql
CREATE TABLE `test_table` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID',
  `_tinyint` tinyint(4) DEFAULT NULL COMMENT '数据类型测试',
  `_smallint` smallint(6) DEFAULT NULL COMMENT '数据类型测试',
  PRIMARY KEY (`id`)
)
```

测试驱动
```xml
<dependency>
    <groupId>com.oceanbase</groupId>
    <artifactId>oceanbase-client</artifactId>
    <version>2.4.2</version>
</dependency>
```

通过mysql驱动读出来的结果

```scala
val sql = "select id, _tinyint, _smallint from test_table"
val driver = "com.mysql.cj.jdbc.Driver"
val url = ""
val df = spark.read.format("jdbc")
  .option("driver", driver)
  .option("url", url)
  .option("user", "xxxxx")
  .option("password", "xxxxx")
  .option("query", query)
  .load()
df.show()
```

```
+---+--------+----+
| id|_tinyint|_int|
+---+--------+----+
|  1|      76|7056|
|  2|      80|1422|
|  5|     107|9387|
|  7|      36|8783|
| 10|      16|9279|
| 11|     101|3674|
| 15|     105|8286|
| 16|      17|3336|
| 18|      48|4650|
| 21|     101| 393|
+---+--------+----+
```

通过 `com.oceanbase.jdbc.Driver` 读取会报错

```scala
val sql = "select id, _tinyint, _smallint from test_table"
val driver = "com.oceanbase.jdbc.Driver"
val url = ""
val df = spark.read.format("jdbc")
  .option("driver", driver)
  .option("url", url)
  .option("user", "xxxxx")
  .option("password", "xxxxx")
  .option("query", query)
  .load()
df.show()
```

```
Caused by: java.sql.SQLException: Bad format for BigDecimal 'id'
	at com.oceanbase.jdbc.internal.com.read.resultset.rowprotocol.TextRowProtocol.getInternalBigDecimal(TextRowProtocol.java:557)
	at com.oceanbase.jdbc.JDBC4ResultSet.getBigDecimal(JDBC4ResultSet.java:1453)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$3(JdbcUtils.scala:418)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$3$adapted(JdbcUtils.scala:416)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:367)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:349)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
```

```scala
val sql = "select _tinyint, _smallint from test_table"
val driver = "com.oceanbase.jdbc.Driver"
val url = ""
val df = spark.read.format("jdbc")
  .option("driver", driver)
  .option("url", url)
  .option("user", "xxxxx")
  .option("password", "xxxxx")
  .option("query", query)
  .load()
df.show()
```

```
Caused by: java.sql.SQLException: Out of range value for column '_tinyint' : value _tinyint
	at com.oceanbase.jdbc.internal.com.read.resultset.rowprotocol.TextRowProtocol.getInternalLongUtil(TextRowProtocol.java:392)
	at com.oceanbase.jdbc.internal.com.read.resultset.rowprotocol.TextRowProtocol.getInternalInt(TextRowProtocol.java:278)
	at com.oceanbase.jdbc.JDBC4ResultSet.getInt(JDBC4ResultSet.java:1366)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:431)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:430)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:367)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:349)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
```